{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ed6d7c7-b190-4216-ac8e-ae3cfda2019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9655302f-4f17-41ee-bc8a-d56c2a77281b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English words/sentences</th>\n",
       "      <th>French words/sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who?</td>\n",
       "      <td>Qui ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Ça alors !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English words/sentences French words/sentences\n",
       "0                     Hi.                 Salut!\n",
       "1                    Run!                Cours !\n",
       "2                    Run!               Courez !\n",
       "3                    Who?                  Qui ?\n",
       "4                    Wow!             Ça alors !"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the dataset path\n",
    "dataset_path = r\"C:\\Users\\faras\\OneDrive\\Desktop\\french.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Display the first few rows to understand the structure\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe2a2ad5-208c-45e6-8439-9ac2e054959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normalizes text by:\n",
    "    1. Lowercasing\n",
    "    2. Removing punctuation\n",
    "    3. Removing extra whitespace\n",
    "    \"\"\"\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Removing punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Removing extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02ea4aa7-176a-4323-b3d3-c68b207cbf80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English words/sentences</th>\n",
       "      <th>French words/sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hi</td>\n",
       "      <td>salut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>run</td>\n",
       "      <td>cours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>run</td>\n",
       "      <td>courez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>who</td>\n",
       "      <td>qui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wow</td>\n",
       "      <td>ça alors</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English words/sentences French words/sentences\n",
       "0                      hi                  salut\n",
       "1                     run                  cours\n",
       "2                     run                 courez\n",
       "3                     who                    qui\n",
       "4                     wow               ça alors"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply normalization to all object (string) columns in the dataset\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    data[column] = data[column].apply(normalize_text)\n",
    "\n",
    "# Display the first few rows after normalization\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b8d3f17-6596-49e8-8e35-9786aaaaa80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized data saved to: C:\\Users\\Nishant\\OneDrive\\Desktop\\normalized_french.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the output path for the normalized data\n",
    "normalized_path = r\"C:\\Users\\Nishant\\OneDrive\\Desktop\\normalized_french.csv\"\n",
    "\n",
    "# Save the normalized dataset to a new CSV file\n",
    "data.to_csv(normalized_path, index=False)\n",
    "\n",
    "print(f\"Normalized data saved to: {normalized_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f51780b-8630-4bf9-953d-eb05f66371b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl.metadata (8.3 kB)\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 524.3/991.5 kB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 991.5/991.5 kB 2.2 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "# Install SentencePiece (uncomment if not already installed)\n",
    "!pip install sentencepiece\n",
    "\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce82218b-6c09-481f-80d8-5cac51fb91a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English words/sentences</th>\n",
       "      <th>French words/sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who?</td>\n",
       "      <td>Qui ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Ça alors !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English words/sentences French words/sentences\n",
       "0                     Hi.                 Salut!\n",
       "1                    Run!                Cours !\n",
       "2                    Run!               Courez !\n",
       "3                    Who?                  Qui ?\n",
       "4                    Wow!             Ça alors !"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the dataset path\n",
    "dataset_path = r\"C:\\Users\\Nishant\\OneDrive\\Desktop\\french.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Display the first few rows\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b901cee-7937-41bd-87dc-ecf52c8d3951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text prepared for SentencePiece at: C:\\Users\\Nishant\\OneDrive\\Desktop\\raw_text.txt\n"
     ]
    }
   ],
   "source": [
    "# Combine all text into a single file for training SentencePiece\n",
    "raw_text_path = r\"C:\\Users\\Nishant\\OneDrive\\Desktop\\raw_text.txt\"\n",
    "with open(raw_text_path, 'w', encoding='utf-8') as f:\n",
    "    for column in data.select_dtypes(include=['object']).columns:\n",
    "        f.write('\\n'.join(data[column]) + '\\n')\n",
    "\n",
    "print(f\"Raw text prepared for SentencePiece at: {raw_text_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd5dd339-2cef-49f2-87cf-52c8bb702d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentencePiece model trained and saved with prefix 'spm_model'.\n"
     ]
    }
   ],
   "source": [
    "# # Train the SentencePiece model\n",
    "# spm.SentencePieceTrainer.train(\n",
    "#     input=raw_text_path, \n",
    "#     model_prefix='spm_model', \n",
    "#     vocab_size=8000,  # Define the vocabulary size\n",
    "#     model_type='bpe'  # 'bpe' for Byte Pair Encoding\n",
    "# )\n",
    "# \n",
    "# print(\"SentencePiece model trained and saved with prefix 'spm_model'.\")\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2909f57d-3baf-4e03-b40f-2ad06a627999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "sp = spm.SentencePieceProcessor(model_file='spm_model.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86d07845-98c3-4007-b63c-7f48254710e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English words/sentences</th>\n",
       "      <th>French words/sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>▁H ▁i ▁ .</td>\n",
       "      <td>▁S ▁al ▁ ut ▁!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>▁R ▁un ▁!</td>\n",
       "      <td>▁C ▁ours ▁!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>▁R ▁un ▁!</td>\n",
       "      <td>▁C ▁ou ▁re z ▁!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>▁Who ▁?</td>\n",
       "      <td>▁Qui ▁?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>▁W ▁ow ▁!</td>\n",
       "      <td>▁Ça ▁alors ▁!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English words/sentences French words/sentences\n",
       "0               ▁H ▁i ▁ .         ▁S ▁al ▁ ut ▁!\n",
       "1               ▁R ▁un ▁!            ▁C ▁ours ▁!\n",
       "2               ▁R ▁un ▁!        ▁C ▁ou ▁re z ▁!\n",
       "3                 ▁Who ▁?                ▁Qui ▁?\n",
       "4               ▁W ▁ow ▁!          ▁Ça ▁alors ▁!"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply SentencePiece tokenization to all text columns\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    data[column] = data[column].apply(lambda x: ' '.join(sp.encode_as_pieces(x)))\n",
    "\n",
    "# Display the first few rows of the tokenized dataset\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c57f039-81a0-466a-b6b0-65d2bb7f0758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized data saved to: C:\\Users\\Nishant\\OneDrive\\Desktop\\subword_tokenized_french.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the output path for the tokenized data\n",
    "tokenized_path = r\"C:\\Users\\Nishant\\OneDrive\\Desktop\\subword_tokenized_french.csv\"\n",
    "\n",
    "# Save the tokenized dataset to a new CSV file\n",
    "data.to_csv(tokenized_path, index=False)\n",
    "\n",
    "print(f\"Tokenized data saved to: {tokenized_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ebc9596-4ef2-451e-ba9b-a6f93adec63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data with numbers handled saved to: C:\\Users\\Nishant\\OneDrive\\Desktop\\processed_french.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Define the dataset path on your system\n",
    "file_path = r\"C:\\Users\\Nishant\\OneDrive\\Desktop\\french.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define function to replace numbers with <NUM>\n",
    "def handle_numbers(text):\n",
    "    return re.sub(r'\\d+', '<NUM>', text)\n",
    "\n",
    "# Apply the function to all columns in the dataset\n",
    "for column in data.columns:\n",
    "    data[column] = data[column].apply(handle_numbers)\n",
    "\n",
    "# Save the processed dataset\n",
    "processed_path = r\"C:\\Users\\Nishant\\OneDrive\\Desktop\\processed_french.csv\"\n",
    "data.to_csv(processed_path, index=False)\n",
    "\n",
    "print(f\"Processed data with numbers handled saved to: {processed_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a83945a3-1fdf-42dc-af03-346200160a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4407673e-4f63-4e59-87f1-c1dd62bf7fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English words/sentences</th>\n",
       "      <th>French words/sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who?</td>\n",
       "      <td>Qui ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Ça alors !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English words/sentences French words/sentences\n",
       "0                     Hi.                 Salut!\n",
       "1                    Run!                Cours !\n",
       "2                    Run!               Courez !\n",
       "3                    Who?                  Qui ?\n",
       "4                    Wow!             Ça alors !"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the dataset path\n",
    "file_path = r\"C:\\Users\\Nishant\\OneDrive\\Desktop\\french.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb16c7d3-8fb4-4d1a-92a8-29233fd65f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 175621 rows\n",
      "Aligned dataset size: 175621 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English words/sentences</th>\n",
       "      <th>French words/sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who?</td>\n",
       "      <td>Qui ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Ça alors !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English words/sentences French words/sentences\n",
       "0                     Hi.                 Salut!\n",
       "1                    Run!                Cours !\n",
       "2                    Run!               Courez !\n",
       "3                    Who?                  Qui ?\n",
       "4                    Wow!             Ça alors !"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows where either the source or target column has missing values\n",
    "aligned_data = data.dropna()\n",
    "\n",
    "# Display information about the aligned dataset\n",
    "print(f\"Original dataset size: {data.shape[0]} rows\")\n",
    "print(f\"Aligned dataset size: {aligned_data.shape[0]} rows\")\n",
    "aligned_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e6042fb-888b-44c8-840a-6eaa59f36b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned parallel corpus saved to: C:\\Users\\Nishant\\OneDrive\\Desktop\\aligned_corpus.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the aligned dataset to a new file\n",
    "aligned_path = r\"C:\\Users\\Nishant\\OneDrive\\Desktop\\aligned_corpus.csv\"\n",
    "aligned_data.to_csv(aligned_path, index=False)\n",
    "\n",
    "print(f\"Aligned parallel corpus saved to: {aligned_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c87e2e49-3678-4541-b4e0-d9312ae0c4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.3)\n",
      "Collecting langdetect\n",
      "  Using cached langdetect-1.0.9.tar.gz (981 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pyspellchecker\n",
      "  Using cached pyspellchecker-0.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Using cached pyspellchecker-0.8.1-py3-none-any.whl (6.8 MB)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993254 sha256=7b401e81ca1a144bf884e9e4f13534599f6342bed90433a55b4caa9321b3867e\n",
      "  Stored in directory: c:\\users\\nishant\\appdata\\local\\pip\\cache\\wheels\\0a\\f2\\b2\\e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
      "Successfully built langdetect\n",
      "Installing collected packages: pyspellchecker, langdetect\n",
      "Successfully installed langdetect-1.0.9 pyspellchecker-0.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas langdetect pyspellchecker\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c521ca9-349c-4bfa-9d57-a6e952e894c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['English words/sentences', 'French words/sentences'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4102bce8-a066-4ada-b0df-3f7f7f12acd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The file C:\\Users\\Nishant\\Desktop\\french.csv does not exist. Please check the path.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the file path (update this with your actual file path)\n",
    "file_path = r\"C:\\Users\\Nishant\\Desktop\\french.csv\"  # Ensure the path is correct\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    print(\"File found, loading dataset...\")\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(file_path)\n",
    "else:\n",
    "    print(f\"Error: The file {file_path} does not exist. Please check the path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87e828cf-3b22-4984-816b-051c6db25063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found, loading dataset...\n",
      "Dataset loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the correct file path\n",
    "file_path = r\"C:\\Users\\Nishant\\OneDrive\\Desktop\\french.csv\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    print(\"File found, loading dataset...\")\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "else:\n",
    "    print(f\"Error: The file {file_path} does not exist. Please check the path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e35e6813-6faf-4085-ab45-05a2228c4b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Language detection\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "# Spell-checking\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Deep learning frameworks\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "DetectorFactory.seed = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77e49b10-d4bf-485d-b5a5-e9578735e69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  source      target\n",
      "0    Hi.      Salut!\n",
      "1   Run!     Cours !\n",
      "2   Run!    Courez !\n",
      "3   Who?       Qui ?\n",
      "4   Wow!  Ça alors !\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "file_path = r\"C:\\Users\\Nishant\\OneDrive\\Desktop\\french.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Rename columns for consistency\n",
    "data.rename(columns={\n",
    "    'English words/sentences': 'source', \n",
    "    'French words/sentences': 'target'\n",
    "}, inplace=True)\n",
    "\n",
    "# Display dataset preview\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "317758a8-943f-498f-bff8-e7c22ee785c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size after removing duplicates: 175621\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate rows\n",
    "data = data.drop_duplicates()\n",
    "print(f\"Dataset size after removing duplicates: {len(data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff432e39-8b4d-4238-955a-e1fa434ba64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: six in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langdetect) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72ab53b9-3a6d-407a-91d7-3f264dc4acae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cf6d7d5-7d73-43ed-bc6d-73f0e74434d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: en\n"
     ]
    }
   ],
   "source": [
    "# Example text for language detection\n",
    "text = \"This is a simple sentence to detect language.\"\n",
    "\n",
    "# Detect the language\n",
    "language = detect(text)\n",
    "\n",
    "# Print the detected language\n",
    "print(\"Detected language:\", language)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82505f53-ee66-4281-8837-cd71e1cbcc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: This is a simple sentence to detect language.\n",
      "Detected Language: en\n",
      "\n",
      "Text: Ceci est une phrase simple pour détecter la langue.\n",
      "Detected Language: fr\n",
      "\n",
      "Text: Dies ist ein einfacher Satz, um die Sprache zu erkennen.\n",
      "Detected Language: de\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"This is a simple sentence to detect language.\",\n",
    "    \"Ceci est une phrase simple pour détecter la langue.\",\n",
    "    \"Dies ist ein einfacher Satz, um die Sprache zu erkennen.\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    print(f\"Text: {text}\\nDetected Language: {detect(text)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1557d73-783d-454d-b61b-b77a16b3f5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sentence_length(sentences, min_length=5, max_length=50):\n",
    "    filtered_sentences = [sentence for sentence in sentences if min_length <= len(sentence.split()) <= max_length]\n",
    "    return filtered_sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5243aa19-b7ad-4c1b-beb0-23dc16d7757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "def correct_spelling(sentences):\n",
    "    spell = SpellChecker()\n",
    "    corrected_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        corrected_words = [spell.correction(word) if word not in spell else word for word in words]\n",
    "        corrected_sentences.append(\" \".join(corrected_words))\n",
    "    \n",
    "    return corrected_sentences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f52f85c-2aa7-479e-8a8f-2e26070ea4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_irrelevant_sentences(sentences, keywords):\n",
    "    filtered_sentences = [sentence for sentence in sentences if any(keyword in sentence for keyword in keywords)]\n",
    "    return filtered_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ecc6d7e-65d4-44c7-911a-ee09c463964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def check_consistency(sentences, keyword=\"example\"):\n",
    "    consistent_sentences = [sentence for sentence in sentences if re.search(r'\\b' + keyword + r'\\b', sentence, re.IGNORECASE)]\n",
    "    return consistent_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "352f1e3e-5c6e-4b9a-b9e5-fedf4b1a53b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_noise(sentences):\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # Remove HTML tags and URLs\n",
    "        sentence = re.sub(r'<.*?>', '', sentence)\n",
    "        sentence = re.sub(r'http\\S+', '', sentence)\n",
    "        sentence = re.sub(r'[^a-zA-Z0-9\\s]', '', sentence)  # Removing special characters\n",
    "        cleaned_sentences.append(sentence)\n",
    "    return cleaned_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3148fc79-d251-4574-8e08-7eae8cb216e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def quality_control(sentences, sample_size=3):\n",
    "    sample = random.sample(sentences, sample_size)\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86ae9363-0fb8-40ef-8f9b-9e32da1d2c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(sentences):\n",
    "    sentences = filter_sentence_length(sentences)\n",
    "    sentences = correct_spelling(sentences)\n",
    "    sentences = filter_irrelevant_sentences(sentences, keywords=[\"relevant\", \"important\"])\n",
    "    sentences = check_consistency(sentences, keyword=\"example\")\n",
    "    sentences = remove_noise(sentences)\n",
    "    quality_control_sample = quality_control(sentences)\n",
    "    \n",
    "    return sentences, quality_control_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "072acc9f-426e-4906-a25d-d17e6f657594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def quality_control(sentences, sample_size=3):\n",
    "    # Ensure the sample size doesn't exceed the number of available sentences\n",
    "    sample_size = min(sample_size, len(sentences))\n",
    "    \n",
    "    # Take the sample\n",
    "    sample = random.sample(sentences, sample_size)\n",
    "    \n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "747954a4-8f04-4e42-98a7-3da5607db480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "This is a short sentence.\n",
      "This sentence is way too long and it exceeds the set word limit by a lot.\n",
      "This is an adequate length.\n",
      "\n",
      "Cleaned Data:\n"
     ]
    }
   ],
   "source": [
    "original_data = sentences[:5]  # First 5 original sentences\n",
    "cleaned_data = clean_data(sentences)[0][:5]  # First 5 cleaned sentences\n",
    "\n",
    "print(\"Original Data:\")\n",
    "for sentence in original_data:\n",
    "    print(sentence)\n",
    "\n",
    "print(\"\\nCleaned Data:\")\n",
    "for sentence in cleaned_data:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5875db6e-873f-4433-90eb-1bc972d89d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'empty_sentences': 0, 'invalid_length_sentences': 0, 'spelling_errors': 3, 'noise_in_data': 0, 'removed_irrelevant_sentences': 3}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Function to check if data is cleaned based on various criteria\n",
    "def check_data_cleanliness(original_data, cleaned_data):\n",
    "    cleanliness_report = {}\n",
    "    \n",
    "    # 1. Check if cleaned data has no empty sentences\n",
    "    cleanliness_report['empty_sentences'] = sum(1 for sentence in cleaned_data if len(sentence.strip()) == 0)\n",
    "    \n",
    "    # 2. Check if sentences have appropriate length (e.g., between 5 and 50 words)\n",
    "    def sentence_length_check(sentences):\n",
    "        return sum(1 for sentence in sentences if len(sentence.split()) < 5 or len(sentence.split()) > 50)\n",
    "\n",
    "    cleanliness_report['invalid_length_sentences'] = sentence_length_check(cleaned_data)\n",
    "    \n",
    "    # 3. Check for spelling errors in cleaned data\n",
    "    spell = SpellChecker()\n",
    "    def check_spelling_errors(sentences):\n",
    "        errors = 0\n",
    "        for sentence in sentences:\n",
    "            words = sentence.split()\n",
    "            misspelled = spell.unknown(words)\n",
    "            errors += len(misspelled)\n",
    "        return errors\n",
    "\n",
    "    cleanliness_report['spelling_errors'] = check_spelling_errors(cleaned_data)\n",
    "    \n",
    "    # 4. Check if noise like HTML tags or URLs are present in the cleaned data\n",
    "    def check_noise(sentences):\n",
    "        noise_count = 0\n",
    "        for sentence in sentences:\n",
    "            if re.search(r'<.*?>|http[s]?://\\S+', sentence):  # Looks for HTML tags or URLs\n",
    "                noise_count += 1\n",
    "        return noise_count\n",
    "    \n",
    "    cleanliness_report['noise_in_data'] = check_noise(cleaned_data)\n",
    "    \n",
    "    # 5. Compare if there are sentences removed due to irrelevance\n",
    "    cleanliness_report['removed_irrelevant_sentences'] = len(original_data) - len(cleaned_data)\n",
    "    \n",
    "    return cleanliness_report\n",
    "\n",
    "# Example usage:\n",
    "original_data = [\n",
    "    \"This is a good sentence.\",\n",
    "    \"This sentence has a spelling mistakee.\",\n",
    "    \"www.example.com is a website.\",\n",
    "    \"Too short.\",\n",
    "    \"This sentence is irrelevant and will be removed.\",\n",
    "    \"Another normal sentence that will stay.\"\n",
    "]\n",
    "\n",
    "# Assume `clean_data()` is the function that cleans the data and returns the cleaned version\n",
    "cleaned_data = [\n",
    "    \"This is a good sentence.\",\n",
    "    \"This sentence has a spelling mistakee.\",\n",
    "    \"Another normal sentence that will stay.\"\n",
    "]\n",
    "\n",
    "# Check the cleanliness of the data\n",
    "cleanliness_report = check_data_cleanliness(original_data, cleaned_data)\n",
    "print(cleanliness_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d608400-0e75-4d2e-a6a2-94e0295b0f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Function to correct spelling errors in a sentence\n",
    "def correct_spelling(sentences):\n",
    "    spell = SpellChecker()\n",
    "    corrected_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()  # Split sentence into words\n",
    "        corrected_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            # Check if the word is spelled correctly\n",
    "            corrected_word = spell.correction(word)  # Get the most likely correction\n",
    "            corrected_words.append(corrected_word)\n",
    "        \n",
    "        corrected_sentence = ' '.join(corrected_words)  # Rejoin words into a sentence\n",
    "        corrected_sentences.append(corrected_sentence)\n",
    "    \n",
    "    return corrected_sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec9fb5c9-9a3a-4926-89b4-8acc01e01a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original spelling errors: 4\n",
      "Corrected spelling errors: 0\n",
      "Spelling errors have been successfully corrected.\n"
     ]
    }
   ],
   "source": [
    "# 1. Count spelling errors before correction\n",
    "original_error_count = count_spelling_errors(sentences)\n",
    "\n",
    "# 2. Correct spelling errors\n",
    "corrected_sentences = correct_spelling(sentences)\n",
    "\n",
    "# 3. Count spelling errors after correction\n",
    "corrected_error_count = count_spelling_errors(corrected_sentences)\n",
    "\n",
    "# 4. Compare the results\n",
    "print(f\"Original spelling errors: {original_error_count}\")\n",
    "print(f\"Corrected spelling errors: {corrected_error_count}\")\n",
    "\n",
    "if original_error_count > corrected_error_count:\n",
    "    print(\"Spelling errors have been successfully corrected.\")\n",
    "else:\n",
    "    print(\"No significant improvement in spelling error correction.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "517ba492-0505-4b59-9056-f856258eec47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: transformers in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: torch in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.11.8)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.18.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas transformers datasets torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3029965-c53c-448a-a899-a5536fcc7da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  English words/sentences French words/sentences\n",
      "0                     Hi.                 Salut!\n",
      "1                    Run!                Cours !\n",
      "2                    Run!               Courez !\n",
      "3                    Who?                  Qui ?\n",
      "4                    Wow!             Ça alors !\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the CSV file\n",
    "data_path = r\"C:\\Users\\Nishant\\OneDrive\\Desktop\\french.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Preview the data\n",
    "print(df.head())\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.train_test_split(test_size=0.1)  # Split into train (90%) and test (10%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c90b4756-5c0d-42a6-8cee-d169c9aa059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load pre-trained tokenizer and model\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91cd0861-0384-4135-82f3-0f73410b0dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['English words/sentences', 'French words/sentences'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "588f7935-3618-4c0b-95a8-66e071771fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Use the correct column names\n",
    "    inputs = examples['English words/sentences']  # Source language column\n",
    "    targets = examples['French words/sentences']  # Target language column\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, truncation=True, padding=True)\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fddde9e-538a-4e64-8af8-82466e04613e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca08a912fca4167865a1569888095ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/158058 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c28862a47cd42769fe921db2569ee6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17563 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6de84ec-6b94-4d74-9c34-c345c791265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = tokenized_datasets['train']\n",
    "test_dataset = tokenized_datasets['test']\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b761115-910a-422c-a0b7-b704f4b8c4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: torch in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.26.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffc187f1-1dba-4be0-9af0-a54a74b6b76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "003ae3f2-16e0-472d-a656-1d57fb8c32cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  English words/sentences     French words/sentences\n",
      "0         I am a shy boy.  Je suis un garçon timide.\n",
      "1         I am in a spot.    Je suis dans le pétrin.\n",
      "2         I am in a spot.   Je suis dans un endroit.\n",
      "3         I had to do it.     Il m'a fallu le faire.\n",
      "4         I saw it on TV.      Je l'ai vu à la télé.\n",
      "Column names: ['English words/sentences', 'French words/sentences']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and inspect the cleaned data\n",
    "data_path = r\"C:\\Users\\Nishant\\cleaned_both_languages.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())\n",
    "\n",
    "# Display column names\n",
    "print(\"Column names:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf3ba73c-aba2-4d75-af2d-0828432cf61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: regex in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sacremoses) (2024.11.6)\n",
      "Collecting click (from sacremoses)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sacremoses) (1.4.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sacremoses) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->sacremoses) (0.4.6)\n",
      "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "   ---------------------------------------- 0.0/897.5 kB ? eta -:--:--\n",
      "   ----------------------- ---------------- 524.3/897.5 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 897.5/897.5 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Installing collected packages: click, sacremoses\n",
      "Successfully installed click-8.1.7 sacremoses-0.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sacremoses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98bf1979-7945-4a3e-a5dc-c5efee501c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sacremoses installed successfully!\n"
     ]
    }
   ],
   "source": [
    "import sacremoses\n",
    "print(\"Sacremoses installed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c91a79d8-b598-4d81-b853-79b1c0ef5660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_path = r\"C:\\Users\\Nishant\\cleaned_both_languages.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Extract source and target texts\n",
    "source_texts = df['English words/sentences'].tolist()  # Source: English\n",
    "target_texts = df['French words/sentences'].tolist()   # Target: French\n",
    "\n",
    "# Load tokenizer for English-to-French translation\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")  # Example: English-to-French model\n",
    "\n",
    "# Tokenize source and target texts\n",
    "source_encodings = tokenizer(source_texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "target_encodings = tokenizer(target_texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d75c22c-ad05-41ca-96ca-5bfa2718eb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, source_encodings, target_encodings):\n",
    "        self.source_encodings = source_encodings\n",
    "        self.target_encodings = target_encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_encodings['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source_item = {key: val[idx] for key, val in self.source_encodings.items()}\n",
    "        target_item = {key: val[idx] for key, val in self.target_encodings.items()}\n",
    "        source_item['labels'] = target_item['input_ids']\n",
    "        return source_item\n",
    "\n",
    "dataset = TranslationDataset(source_encodings, target_encodings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "589265b7-9f66-4921-b668-2c91e56ccc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81b98b4c-be86-42b5-8fff-7e71d60dbf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3303bada-e8a3-4c75-90a0-cbf57fd95f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Seq2SeqTrainingArguments\n",
    "# \n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\"./results\",             # Save model checkpoints\n",
    "#     evaluation_strategy=\"epoch\",       # Evaluate model at the end of each epoch\n",
    "#     learning_rate=5e-5,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "#     save_total_limit=2,\n",
    "#     predict_with_generate=True,        # Enable text generation during evaluation\n",
    "#     logging_dir='./logs',              # Log directory\n",
    "#     logging_steps=10,\n",
    "# )\n",
    "# \n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61b1a9fe-fcb8-4f8e-9291-474229fe148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Trainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "# \n",
    "# # Set up training arguments (as before)\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\"./results\",             # Save model checkpoints\n",
    "#     eval_strategy=\"epoch\",              # Evaluate model at the end of each epoch\n",
    "#     learning_rate=5e-5,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "#     save_total_limit=2,\n",
    "#     predict_with_generate=True,         # Enable text generation during evaluation\n",
    "#     logging_dir='./logs',               # Log directory\n",
    "#     logging_steps=10,\n",
    "# )\n",
    "# \n",
    "# # Create the data collator (handles tokenization and padding)\n",
    "# data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "# \n",
    "# # Create the Trainer instance without passing `tokenizer`\n",
    "# trainer = Trainer(\n",
    "#     model=model,                        # The model to train\n",
    "#     args=training_args,                 # The training arguments\n",
    "#     train_dataset=dataset,              # The training dataset\n",
    "#     eval_dataset=dataset,               # The evaluation dataset (or validation set)\n",
    "#     data_collator=data_collator,        # Data collator handles tokenization and padding\n",
    "#     processing_class=DataCollatorForSeq2Seq  # NEW: Add this line for processing class (no tokenizer needed)\n",
    "# )\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36487553-1854-41c9-993e-268d3772a914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85bfb65216a049f2a90448e59028d706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1e3214f7314510b33a5e743085d68f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'English words/sentences': 'Hello', 'Target': 'Hola', 'input_ids': [101, 7592, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'English words/sentences': 'What is your name?', 'Target': '¿Cuál es tu nombre?', 'input_ids': [101, 2054, 2003, 2115, 2171, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "# Load the tokenizer (you can use a different model as required)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Assuming 'dataset' is a dictionary containing 'train' and 'test' splits, \n",
    "# and both 'train' and 'test' are pandas DataFrames.\n",
    "\n",
    "# Convert pandas DataFrames to Hugging Face Datasets\n",
    "train_df = pd.DataFrame({\n",
    "    'English words/sentences': ['Hello', 'How are you?', 'Goodbye'],  # Example data\n",
    "    'Target': ['Hola', '¿Cómo estás?', 'Adiós']  # Example translations\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'English words/sentences': ['What is your name?', 'Where are you from?'],\n",
    "    'Target': ['¿Cuál es tu nombre?', '¿De dónde eres?']\n",
    "})\n",
    "\n",
    "# Convert DataFrames to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Create a DatasetDict object\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['English words/sentences'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Apply tokenization with map() function (batch-wise)\n",
    "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)\n",
    "\n",
    "# Inspect the tokenized datasets\n",
    "print(tokenized_datasets['train'][0])\n",
    "print(tokenized_datasets['test'][0])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "43d9c1ae-cb90-4833-aaa3-c9a3552deec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['English words/sentences', 'Target'],\n",
      "    num_rows: 3\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load your data (make sure this is a valid DataFrame)\n",
    "train_df = pd.DataFrame({\n",
    "    \"English words/sentences\": [\"Hello\", \"How are you?\", \"Good morning\"],\n",
    "    \"Target\": [\"Hola\", \"¿Cómo estás?\", \"Buenos días\"]\n",
    "})\n",
    "test_df = pd.DataFrame({\n",
    "    \"English words/sentences\": [\"Good night\", \"Thank you\", \"Goodbye\"],\n",
    "    \"Target\": [\"Buenas noches\", \"Gracias\", \"Adiós\"]\n",
    "})\n",
    "\n",
    "# Convert DataFrame to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Check the dataset structure\n",
    "print(train_dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d91ae107-14db-46ff-8415-02edc329aadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63918cf83d144199a8f287f6ecdbb377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b39ce1037a451888dc0386f898a68b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'English words/sentences': 'Hello', 'Target': 'Hola', 'input_ids': [101, 7592, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [101, 7570, 2721, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer (BERT example)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenization function that tokenizes both input and target columns\n",
    "def tokenize_function(examples):\n",
    "    source_encodings = tokenizer(examples['English words/sentences'], padding='max_length', truncation=True, max_length=128)\n",
    "    target_encodings = tokenizer(examples['Target'], padding='max_length', truncation=True, max_length=128)\n",
    "    \n",
    "    # Add the target as the 'labels' for training\n",
    "    source_encodings['labels'] = target_encodings['input_ids']\n",
    "    \n",
    "    return source_encodings\n",
    "\n",
    "# Apply the tokenization to the train and test datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Check the tokenized dataset\n",
    "print(train_dataset[0])  # Check a sample after tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea0636f5-ae32-4ddc-8e52-4dc334f19da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'English words/sentences': 'Hello', 'Target': 'Hola', 'input_ids': [101, 7592, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [101, 7570, 2721, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# Create a DataCollator to handle padding\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=None)\n",
    "\n",
    "# Check a sample to confirm the correct padding and labels\n",
    "print(train_dataset[0])  # Check the first sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "226c406d-93e5-48e4-b6e2-6c509569a847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db14c830f0f416abccc5a0e9294794c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nishant\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Nishant\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11171747428a40e99acbfa04205156c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0935979e5c64186ba99cd2d4a9c16a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nishant\\AppData\\Local\\Temp\\ipykernel_70744\\196915900.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 03:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>14.804253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>11.622130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>8.572099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nishant\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=13.149733225504557, metrics={'train_runtime': 229.5207, 'train_samples_per_second': 0.039, 'train_steps_per_second': 0.013, 'total_flos': 2437992677376.0, 'train_loss': 13.149733225504557, 'epoch': 3.0})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
    "# \n",
    "# # Load a pre-trained model (example: BART)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "# \n",
    "# # Set up training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",  # Output directory for model checkpoints\n",
    "#     evaluation_strategy=\"epoch\",  # Evaluate after each epoch\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "# )\n",
    "# \n",
    "# # Initialize Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=test_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "# )\n",
    "# \n",
    "# # Train the model\n",
    "# trainer.train()\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "211e7d6b-49a1-431d-bea8-96462fc4231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af059bef-7c69-46b8-9f00-b06774e44cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5bdd5c86aa54a40a951cc700df3dcfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697c010cc91347b185e81ae27849f26d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 02:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>10.949170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>10.679952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>10.411410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nishant\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=11.25826899210612, metrics={'train_runtime': 156.1462, 'train_samples_per_second': 0.058, 'train_steps_per_second': 0.019, 'total_flos': 2437992677376.0, 'train_loss': 11.25826899210612, 'epoch': 3.0})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os\n",
    "# from transformers import Trainer, TrainingArguments, BartForConditionalGeneration, BartTokenizer\n",
    "# from datasets import Dataset\n",
    "# \n",
    "# # Disable the symlink warning (optional)\n",
    "# os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "# \n",
    "# # Load the dataset (assuming it's a pandas DataFrame)\n",
    "# dataset = {'train': train_df, 'test': test_df}\n",
    "# \n",
    "# # Convert to Hugging Face Dataset format\n",
    "# train_dataset = Dataset.from_pandas(dataset['train'])\n",
    "# test_dataset = Dataset.from_pandas(dataset['test'])\n",
    "# \n",
    "# # Load model and tokenizer\n",
    "# model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "# tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "# \n",
    "# # Set generation parameters\n",
    "# model.config.max_length = 142\n",
    "# model.config.min_length = 56\n",
    "# model.config.num_beams = 4\n",
    "# model.config.length_penalty = 2.0\n",
    "# model.config.no_repeat_ngram_size = 3\n",
    "# model.config.forced_bos_token_id = 0\n",
    "# \n",
    "# # Define the training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=4,\n",
    "#     per_device_eval_batch_size=4,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01\n",
    "# )\n",
    "# \n",
    "# # Tokenization function\n",
    "# def tokenize_function(examples):\n",
    "#     # Tokenize the source and target\n",
    "#     tokenized_inputs = tokenizer(examples['English words/sentences'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "#     tokenized_targets = tokenizer(examples['Target'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "#     \n",
    "#     # Add the 'labels' key to the dictionary\n",
    "#     tokenized_inputs['labels'] = tokenized_targets['input_ids']\n",
    "#     return tokenized_inputs\n",
    "# \n",
    "# # Apply the tokenization\n",
    "# train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "# test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "# \n",
    "# # Initialize the Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=test_dataset\n",
    "# )\n",
    "# \n",
    "# # Train the model\n",
    "# trainer.train()\n",
    "# \n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c2506560-cf24-4f59-baf7-d318663e0db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your input sentence here. Your input sentence of the day will appear at the bottom of this article. Please submit your input sentence by Friday at 8:30 a.m. ET. For confidential support call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch, see www.samaritans.org.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "# \n",
    "# # Load the model and tokenizer\n",
    "# model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "# tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "# \n",
    "# # Set generation parameters in the model's config\n",
    "# model.config.max_length = 142  # Set a reasonable max_length\n",
    "# model.config.min_length = 56\n",
    "# model.config.num_beams = 4\n",
    "# model.config.length_penalty = 2.0\n",
    "# model.config.no_repeat_ngram_size = 3\n",
    "# model.config.forced_bos_token_id = 0\n",
    "# model.config.early_stopping = True\n",
    "# \n",
    "# # Your input sentence\n",
    "# input_text = \"Your input sentence here.\"\n",
    "# \n",
    "# # Tokenize the input text with explicit max_length during tokenization\n",
    "# inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "# \n",
    "# # Generate text (summary, translation, etc.)\n",
    "# generated_ids = model.generate(\n",
    "#     inputs['input_ids'],\n",
    "#     attention_mask=inputs['attention_mask'],\n",
    "#     max_length=142  # Set max_length for generation to control output size\n",
    "# )\n",
    "# \n",
    "# # Decode the generated text\n",
    "# generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "# print(generated_text)\n",
    "# \n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "953c254c-2e53-48bd-94c8-da4dd106c00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nishant\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1399: UserWarning: Unfeasible length constraints: `min_length` (56) is larger than the maximum possible length (50). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: The quick brown fox jumps over the lazy dog is a common phrase used in typing tests. The phrase contains all the letters of the English alphabet, making it useful for testing typewriters, keyboards, and fonts. It is also used\n"
     ]
    }
   ],
   "source": [
    "# from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "# \n",
    "# # Load the pre-trained BART model and tokenizer\n",
    "# model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "# tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "# \n",
    "# # Simple input text (e.g., a news article or a short paragraph)\n",
    "# input_text = \"\"\"\n",
    "# The quick brown fox jumps over the lazy dog. It is a common phrase used in typing tests.\n",
    "# The phrase contains all the letters of the English alphabet, making it useful for testing typewriters, keyboards, and fonts.\n",
    "# \"\"\"\n",
    "# \n",
    "# # Tokenize the input text (with truncation and padding as necessary)\n",
    "# inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "# \n",
    "# # Generate summary with the model\n",
    "# generated_ids = model.generate(\n",
    "#     inputs['input_ids'],\n",
    "#     attention_mask=inputs['attention_mask'],\n",
    "#     max_length=50,  # Maximum length of the summary\n",
    "#     num_beams=4,  # Use beam search for better results\n",
    "#     length_penalty=2.0,  # Penalize longer summaries\n",
    "#     early_stopping=True  # Stop if the summary is complete\n",
    "# )\n",
    "# \n",
    "# # Decode the generated output (summary)\n",
    "# generated_summary = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "# \n",
    "# # Print the summary\n",
    "# print(\"Summary:\", generated_summary)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c6feeb9d-2d9b-413f-a172-c8a267cc92fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0ad505e4-6ea2-418e-9ad4-d088962766dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8b8783a6-d5b5-48d8-b1b4-0de0977f5de7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27eebc46a65a4ec799e1675f07673238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d26789fe18484174a4230d5fa029e356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 02:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>10.410860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>10.418580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>10.258078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nishant\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=10.923394521077475, metrics={'train_runtime': 165.8148, 'train_samples_per_second': 0.018, 'train_steps_per_second': 0.018, 'total_flos': 3250656903168.0, 'train_loss': 10.923394521077475, 'epoch': 3.0})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import BartForConditionalGeneration, Trainer, TrainingArguments\n",
    "# from datasets import Dataset\n",
    "# from transformers import BartTokenizer\n",
    "# \n",
    "# # Step 1: Load the model and tokenizer\n",
    "# model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "# tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "# \n",
    "# # Step 2: Prepare the data (Assume `train_df` is already loaded as a pandas DataFrame)\n",
    "# train_df = {\n",
    "#     \"text\": [\n",
    "#         \"Translate this sentence.\",\n",
    "#         \"This is a second sentence.\"\n",
    "#     ],\n",
    "#     \"target\": [\n",
    "#         \"Traduce esta oración.\",\n",
    "#         \"Esta es una segunda oración.\"\n",
    "#     ]\n",
    "# }\n",
    "# \n",
    "# # Convert to Hugging Face Dataset format\n",
    "# train_dataset = Dataset.from_dict(train_df)\n",
    "# \n",
    "# # Step 3: Split dataset into train and validation sets (80-20 split)\n",
    "# train_dataset, eval_dataset = train_dataset.train_test_split(test_size=0.2).values()\n",
    "# \n",
    "# # Step 4: Tokenize the datasets (Include 'labels' as the target)\n",
    "# def tokenize_function(examples):\n",
    "#     inputs = tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "#     targets = tokenizer(examples['target'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "#     inputs['labels'] = targets['input_ids']  # Add target input_ids as labels\n",
    "#     return inputs\n",
    "# \n",
    "# # Tokenize both the train and eval datasets\n",
    "# train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "# eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "# \n",
    "# # Step 5: Define Training Arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',  # Output directory for model checkpoints\n",
    "#     evaluation_strategy=\"epoch\",  # Evaluate after each epoch\n",
    "#     learning_rate=2e-5,  # Learning rate\n",
    "#     per_device_train_batch_size=8,  # Batch size for training\n",
    "#     per_device_eval_batch_size=8,  # Batch size for evaluation\n",
    "#     num_train_epochs=3,  # Number of training epochs\n",
    "# )\n",
    "# \n",
    "# # Step 6: Initialize Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,  # Tokenized training dataset\n",
    "#     eval_dataset=eval_dataset,  # Tokenized validation dataset\n",
    "# )\n",
    "# \n",
    "# # Step 7: Train the model\n",
    "# trainer.train()\n",
    "# \n",
    "# \n",
    "#  \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ec00b574-0200-4a7f-b196-1d586a1f7670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate this sentence into English. Use the weekly Newsquiz to test your knowledge of stories you saw on CNN.com. Today's News Quiz includes the question, \"What do you know about the U.S. military?\" The answer, of course, is \"I know a lot of things.\"\n"
     ]
    }
   ],
   "source": [
    "# from transformers import BartForConditionalGeneration, BartTokenizer, GenerationConfig\n",
    "# \n",
    "# # Load model and tokenizer\n",
    "# model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "# tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "# \n",
    "# # Create a generation config with the parameters\n",
    "# gen_config = GenerationConfig(\n",
    "#     max_length=142,\n",
    "#     min_length=56,\n",
    "#     early_stopping=True,\n",
    "#     num_beams=4,\n",
    "#     length_penalty=2.0,\n",
    "#     no_repeat_ngram_size=3,\n",
    "#     forced_bos_token_id=0\n",
    "# )\n",
    "# \n",
    "# # Example generation\n",
    "# input_text = \"Translate this sentence\"\n",
    "# input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "# \n",
    "# output = model.generate(input_ids, generation_config=gen_config)\n",
    "# print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4dfafbff-a4e9-4c39-9211-44e417ae1712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 02:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>9.075006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>8.808546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>8.647938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=9.185171127319336, metrics={'train_runtime': 178.4206, 'train_samples_per_second': 0.017, 'train_steps_per_second': 0.017, 'total_flos': 3250656903168.0, 'train_loss': 9.185171127319336, 'epoch': 3.0})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainer.train()\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "858ca716-5151-4035-840d-12964342ace2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.647937774658203, 'eval_runtime': 3.0596, 'eval_samples_per_second': 0.327, 'eval_steps_per_second': 0.327, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# eval_results = trainer.evaluate(eval_dataset)  # Or test_dataset\n",
    "# print(eval_results)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e016f123-2647-4b98-a0f5-dd194456471d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nishant\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./my_model\\\\tokenizer_config.json',\n",
       " './my_model\\\\special_tokens_map.json',\n",
       " './my_model\\\\vocab.json',\n",
       " './my_model\\\\merges.txt',\n",
       " './my_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./my_model')\n",
    "tokenizer.save_pretrained('./my_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d2c0f5c5-11b2-4a4a-9a5d-2244ae0b3b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your input sentence here. Your input sentence of the day will appear at the bottom of this article. Please submit your input sentence by Friday at 8:30 a.m. ET. For confidential support call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch, see www.samaritans.org.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import BartForConditionalGeneration, BartTokenizer, GenerationConfig\n",
    "# \n",
    "# # Load model and tokenizer\n",
    "# model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "# tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "# \n",
    "# # Example of setting generation parameters using GenerationConfig\n",
    "# generation_config = GenerationConfig(\n",
    "#     max_length=142,  # Max length of generated text\n",
    "#     min_length=56,   # Minimum length of generated text\n",
    "#     early_stopping=True,\n",
    "#     num_beams=4,\n",
    "#     length_penalty=2.0,\n",
    "#     no_repeat_ngram_size=3,\n",
    "#     forced_bos_token_id=0\n",
    "# )\n",
    "# \n",
    "# # You can now use generation_config while generating\n",
    "# inputs = tokenizer(\"Your input sentence here.\", return_tensors=\"pt\")\n",
    "# output = model.generate(inputs[\"input_ids\"], generation_config=generation_config)\n",
    "# \n",
    "# # Decode the output\n",
    "# decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "# print(decoded_output)\n",
    "# \n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f99610b6-f737-4f10-af8f-c4d619f31ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path does not exist.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "saved_model_path = './my_saved_model'\n",
    "if os.path.exists(saved_model_path):\n",
    "    print(f\"Model is saved at: {os.path.abspath(saved_model_path)}\")\n",
    "else:\n",
    "    print(\"Model path does not exist.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "15c03ebd-efcd-47cb-82cc-d783c699b214",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nishant\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained('./my_saved_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "85e769e7-b97b-4d09-8a5c-29f25f8d16a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: C:\\Users\\Nishant\\my_saved_model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check if the directory exists\n",
    "model_save_path = './my_saved_model'\n",
    "if os.path.exists(model_save_path):\n",
    "    print(f\"Model saved at: {os.path.abspath(model_save_path)}\")\n",
    "else:\n",
    "    print(f\"Model path '{model_save_path}' does not exist. Please check the path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "eb56efbe-b537-4044-b64e-f4de71715380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# from transformers import BartTokenizer\n",
    "# \n",
    "# # Load the tokenizer from the pre-trained model (before saving it)\n",
    "# tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "# \n",
    "# # Save the tokenizer to the same directory where the model is saved\n",
    "# tokenizer.save_pretrained(model_save_path)\n",
    "# \n",
    "# print(\"Tokenizer saved successfully!\")\n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "acc97880-cdb2-47fe-95bb-6686437708db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "# \n",
    "# # Path to the saved model\n",
    "# model_save_path = r'C:\\Users\\Nishant\\my_saved_model'\n",
    "# \n",
    "# # Load the saved model and tokenizer\n",
    "# model = BartForConditionalGeneration.from_pretrained(model_save_path)\n",
    "# tokenizer = BartTokenizer.from_pretrained(model_save_path)\n",
    "# \n",
    "# print(\"Model and tokenizer loaded successfully!\")\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d7e274c1-9567-4732-9848-501ec115f4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"This is a simple test sentence.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2b711057-820b-405b-9061-9df7d2b1f681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input text with an explicit max_length\n",
    "input_text = \"This is a simple test sentence.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4d7b28d7-3377-423c-9111-4b07097a4491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Text: This is a simple test sentence.\n"
     ]
    }
   ],
   "source": [
    "# Generate the translated output\n",
    "translated_ids = model.generate(inputs['input_ids'], max_length=50, num_beams=4, early_stopping=True)\n",
    "\n",
    "# Decode the output tokens to get the translated text\n",
    "translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Translated Text: {translated_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7fe53563-4ff6-439a-ab2a-74786d5cd553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load pre-trained translation model and tokenizer\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-fr'\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d5dbadd0-36d4-40c8-88d1-4196ce248304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input text (English)\n",
    "input_text = \"This is a simple test sentence.\"\n",
    "\n",
    "# Tokenize the input sentence\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "05377a4a-cbcc-4c6d-b0fa-4c298af9cd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate translation (French)\n",
    "translated_ids = model.generate(inputs['input_ids'], max_length=50, num_beams=4, early_stopping=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "333f4b68-e1bf-423f-a0d9-dd204efad601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Text: C'est une simple phrase d'essai.\n"
     ]
    }
   ],
   "source": [
    "# Decode and print the translated text\n",
    "translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "print(f\"Translated Text: {translated_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "817cb471-9398-4888-9d60-bb4513a0d57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  How are you today?\n",
      "French:  Comment allez-vous aujourd'hui ?\n"
     ]
    }
   ],
   "source": [
    " from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load the MarianMT model and tokenizer for English to French translation\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-fr'\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to translate from English to French\n",
    "def translate_english_to_french(english_sentence):\n",
    "    # Tokenize the input English sentence\n",
    "    inputs = tokenizer(english_sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Generate the translation (French) from the model\n",
    "    translated_ids = model.generate(inputs[\"input_ids\"], max_length=50, num_beams=4, early_stopping=True)\n",
    "\n",
    "    # Decode the translated sentence\n",
    "    french_translation = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return french_translation\n",
    "\n",
    "# Example input\n",
    "english_sentence = \"How are you today?\"\n",
    "\n",
    "# Translate to French\n",
    "french_sentence = translate_english_to_french(english_sentence)\n",
    "\n",
    "print(\"English: \", english_sentence)\n",
    "print(\"French: \", french_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0864b1dd-ef2c-43ec-af11-65294ac54c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  I am a shy boy.\n",
      "French:  Je suis un garçon timide.\n"
     ]
    }
   ],
   "source": [
    " from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load the MarianMT model and tokenizer for English to French translation\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-fr'\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to translate from English to French\n",
    "def translate_english_to_french(english_sentence):\n",
    "    # Tokenize the input English sentence\n",
    "    inputs = tokenizer(english_sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Generate the translation (French) from the model\n",
    "    translated_ids = model.generate(inputs[\"input_ids\"], max_length=50, num_beams=4, early_stopping=True)\n",
    "\n",
    "    # Decode the translated sentence\n",
    "    french_translation = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return french_translation\n",
    "\n",
    "# Example input\n",
    "english_sentence = \"I am a shy boy.\"\n",
    "\n",
    "# Translate to French\n",
    "french_sentence = translate_english_to_french(english_sentence)\n",
    "\n",
    "print(\"English: \", english_sentence)\n",
    "print(\"French: \", french_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7309bdce-17b3-4ee5-b622-ec22d63aa047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MarianConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"Helsinki-NLP/opus-mt-en-fr\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      59513\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 59513,\n",
      "  \"decoder_vocab_size\": 59514,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 59513,\n",
      "  \"scale_embedding\": true,\n",
      "  \"share_encoder_decoder_embeddings\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 59514\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bd17d3c-2879-493d-9a20-ad6f5f2d052a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  I hate you.\n",
      "French:  Je te déteste.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load the MarianMT model and tokenizer for English to French translation\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-fr'\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to translate from English to French\n",
    "def translate_english_to_french(english_sentence):\n",
    "    # Tokenize the input English sentence\n",
    "    inputs = tokenizer(english_sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Generate the translation (French) from the model\n",
    "    translated_ids = model.generate(inputs[\"input_ids\"], max_length=50, num_beams=4, early_stopping=True)\n",
    "\n",
    "    # Decode the translated sentence\n",
    "    french_translation = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return french_translation\n",
    "\n",
    "# Example input\n",
    "english_sentence = \"I hate you.\"\n",
    "\n",
    "# Translate to French\n",
    "french_sentence = translate_english_to_french(english_sentence)\n",
    "\n",
    "print(\"English: \", english_sentence)\n",
    "print(\"French: \", french_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "761232f1-a1d7-4c10-8763-2d43e924ef4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  I miss you.\n",
      "French:  Tu me manques.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load the MarianMT model and tokenizer for English to French translation\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-fr'\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to translate from English to French\n",
    "def translate_english_to_french(english_sentence):\n",
    "    # Tokenize the input English sentence\n",
    "    inputs = tokenizer(english_sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Generate the translation (French) from the model\n",
    "    translated_ids = model.generate(inputs[\"input_ids\"], max_length=50, num_beams=4, early_stopping=True)\n",
    "\n",
    "    # Decode the translated sentence\n",
    "    french_translation = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return french_translation\n",
    "\n",
    "# Example input\n",
    "english_sentence = \"I miss you.\"\n",
    "\n",
    "# Translate to French\n",
    "french_sentence = translate_english_to_french(english_sentence)\n",
    "\n",
    "print(\"English: \", english_sentence)\n",
    "print(\"French: \", french_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "618dd36f-4ea4-48ab-ac43-0f4e3b0dfa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  My name is Aditya.\n",
      "French:  Mon nom est Aditya.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load the MarianMT model and tokenizer for English to French translation\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-fr'\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to translate from English to French\n",
    "def translate_english_to_french(english_sentence):\n",
    "    # Tokenize the input English sentence\n",
    "    inputs = tokenizer(english_sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Generate the translation (French) from the model\n",
    "    translated_ids = model.generate(inputs[\"input_ids\"], max_length=50, num_beams=4, early_stopping=True)\n",
    "\n",
    "    # Decode the translated sentence\n",
    "    french_translation = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return french_translation\n",
    "\n",
    "# Example input\n",
    "english_sentence = \"My name is Aditya.\"\n",
    "\n",
    "# Translate to French\n",
    "french_sentence = translate_english_to_french(english_sentence)\n",
    "\n",
    "print(\"English: \", english_sentence)\n",
    "print(\"French: \", french_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6880eac-f00d-42ef-a1c0-ef31e71c32b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unused cells (containing BART-related code): [9, 49, 57, 58, 63, 65, 66, 67, 69, 70, 71, 72, 73, 75, 79, 80, 94]\n"
     ]
    }
   ],
   "source": [
    "# import nbformat\n",
    "# \n",
    "# # Path to your notebook\n",
    "# notebook_path = 'Untitled1.ipynb'  # Replace with the actual notebook path if not in the same directory\n",
    "# \n",
    "# # Load the notebook file\n",
    "# with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "#     notebook_content = nbformat.read(f, as_version=4)\n",
    "# \n",
    "# # List of keywords related to BART and other models you want to check\n",
    "# keywords = ['BartForConditionalGeneration', 'BartTokenizer', 'facebook/bart', 'Trainer', 'TrainingArguments']\n",
    "# \n",
    "# # Initialize a list to store the indices of irrelevant cells (containing BART code)\n",
    "# unused_cells = []\n",
    "# \n",
    "# # Iterate through all cells in the notebook\n",
    "# for idx, cell in enumerate(notebook_content.cells):\n",
    "#     if cell.cell_type == \"code\":  # Check only code cells\n",
    "#         code_content = cell.source\n",
    "#         # Check if any of the keywords are found in the code content\n",
    "#         if any(keyword.lower() in code_content.lower() for keyword in keywords):\n",
    "#             unused_cells.append(idx + 1)  # Store the kernel number (Jupyter starts counting from 1)\n",
    "# \n",
    "# # Output the kernel numbers of cells containing BART code\n",
    "# print(\"Unused cells (containing BART-related code):\", unused_cells)\n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bc6d20f-979b-4786-8204-d9e66d9d7c12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  I am sad.\n",
      "French:  Je suis triste.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load the MarianMT model and tokenizer for English to French translation\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-fr'\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to translate from English to French\n",
    "def translate_english_to_french(english_sentence):\n",
    "    # Tokenize the input English sentence\n",
    "    inputs = tokenizer(english_sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Generate the translation (French) from the model\n",
    "    translated_ids = model.generate(inputs[\"input_ids\"], max_length=50, num_beams=4, early_stopping=True)\n",
    "\n",
    "    # Decode the translated sentence\n",
    "    french_translation = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return french_translation\n",
    "\n",
    "# Example input\n",
    "english_sentence = \"I am sad.\"\n",
    "\n",
    "# Translate to French\n",
    "french_sentence = translate_english_to_french(english_sentence)\n",
    "\n",
    "print(\"English: \", english_sentence)\n",
    "print(\"French: \", french_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f6cdc04-be51-4c16-957c-4f37ef17d3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['English words/sentences', 'French words/sentences'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('C:/Users/Nishant/cleaned_both_languages.csv')\n",
    "\n",
    "# Print the column names\n",
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d63a61f6-cdb3-49e1-a778-a214d8a2bdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The English sentence is not present in the dataset.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('C:/Users/Nishant/cleaned_both_languages.csv')\n",
    "\n",
    "# Define the English sentence to search for\n",
    "english_input = \"Technology has transformed the way we communicate and work. It has made it possible for people from different parts of the world to connect instantly. With the rapid advancements in artificial intelligence, the future promises even greater innovations. However, it is important to balance technological progress with social and ethical considerations to ensure a better future for everyone.\"\n",
    "\n",
    "# Check if the sentence exists in the 'English words/sentences' column of your dataset\n",
    "if english_input in data['English words/sentences'].values:\n",
    "    print(\"The English sentence is present in the dataset.\")\n",
    "else:\n",
    "    print(\"The English sentence is not present in the dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cc03c10-a15e-4378-a040-eaf54f984795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee43e209-8b5f-40ee-980b-a50fbc9f0b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus BLEU Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Ground truth (reference translation) and model output (hypothesis)\n",
    "references = [[\"mon nom est aditya\".split()]]  # Reference: Human-translated sentence\n",
    "hypotheses = [\"mon nom est aditya\".split()]   # Hypothesis: Model-generated sentence\n",
    "\n",
    "# Compute BLEU score\n",
    "bleu_score = corpus_bleu(references, hypotheses)\n",
    "print(f\"Corpus BLEU Score: {bleu_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faefae82-f1d8-4422-b232-b5b75daa4f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus BLEU Score: 0.0627\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"C:\\Users\\Nishant\\OneDrive\\Desktop\\french.csv\"\n",
    "dataset = pd.read_csv(file_path)\n",
    "\n",
    "# Load MarianMT model and tokenizer\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-fr'\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to translate a small batch of sentences\n",
    "def translate(sentences):\n",
    "    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    translated = model.generate(inputs['input_ids'], max_length=512, num_beams=4, early_stopping=True)\n",
    "    return [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "\n",
    "# Limit to a small subset for testing\n",
    "subset = dataset['English words/sentences'][:10]  # First 10 sentences for testing\n",
    "\n",
    "# Generate translations for the subset\n",
    "model_outputs = translate(list(subset))\n",
    "\n",
    "# Prepare references (ground truth translations)\n",
    "references = [[ref.split()] for ref in dataset['French words/sentences'][:10]]\n",
    "\n",
    "# Prepare hypotheses (model-generated translations)\n",
    "hypotheses = [output.split() for output in model_outputs]\n",
    "\n",
    "# Use SmoothingFunction to prevent zero BLEU scores\n",
    "smooth = SmoothingFunction().method4\n",
    "\n",
    "# Calculate BLEU score with smoothing\n",
    "bleu_score = corpus_bleu(references, hypotheses, smoothing_function=smooth)\n",
    "print(f\"Corpus BLEU Score: {bleu_score:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd2edf7e-3606-4f9f-abcc-39dab13f0798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Data:\n",
      "   English words/sentences French words/sentences English Tokens  \\\n",
      "0                     Hi.                 Salut!         ['hi']   \n",
      "1                    Run!                Cours !        ['run']   \n",
      "2                    Run!               Courez !        ['run']   \n",
      "3                    Who?                  Qui ?        ['who']   \n",
      "4                    Wow!             Ça alors !        ['wow']   \n",
      "\n",
      "    French Tokens  \n",
      "0       ['salut']  \n",
      "1       ['cours']  \n",
      "2      ['courez']  \n",
      "3         ['qui']  \n",
      "4  ['a', 'alors']  \n",
      "\n",
      "Cleaned Data:\n",
      "   English words/sentences     French words/sentences\n",
      "0         I am a shy boy.  Je suis un garçon timide.\n",
      "1         I am in a spot.    Je suis dans le pétrin.\n",
      "2         I am in a spot.   Je suis dans un endroit.\n",
      "3         I had to do it.     Il m'a fallu le faire.\n",
      "4         I saw it on TV.      Je l'ai vu à la télé.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load preprocessed data files\n",
    "normalized_data = pd.read_csv(r'C:\\Users\\Nishant\\OneDrive\\Desktop\\normalized_tokenized_data.csv')\n",
    "cleaned_data = pd.read_csv(r'C:\\Users\\Nishant\\cleaned_both_languages.csv')\n",
    "\n",
    "# Display the first few rows of each to verify\n",
    "print(\"Normalized Data:\\n\", normalized_data.head())\n",
    "print(\"\\nCleaned Data:\\n\", cleaned_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1d8f4cf-f425-44bd-a8d1-97b27d7f9ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus BLEU Score: 0.7821\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load cleaned data\n",
    "file_path_cleaned = r'C:\\Users\\Nishant\\cleaned_both_languages.csv'\n",
    "dataset = pd.read_csv(file_path_cleaned)\n",
    "\n",
    "# Load MarianMT model and tokenizer\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-fr'\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to translate a batch of sentences\n",
    "def translate(sentences):\n",
    "    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    translated = model.generate(inputs['input_ids'], max_length=512, num_beams=4, early_stopping=True)\n",
    "    return [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "\n",
    "# Generate translations for the first 10 sentences for testing\n",
    "subset = dataset['English words/sentences'][:10]\n",
    "model_outputs = translate(list(subset))\n",
    "\n",
    "# Prepare references (ground truth translations)\n",
    "references = [[ref.split()] for ref in dataset['French words/sentences'][:10]]\n",
    "\n",
    "# Prepare hypotheses (model-generated translations)\n",
    "hypotheses = [output.split() for output in model_outputs]\n",
    "\n",
    "# Use SmoothingFunction to prevent zero BLEU scores\n",
    "smooth = SmoothingFunction().method4\n",
    "\n",
    "# Calculate BLEU score with smoothing\n",
    "bleu_score = corpus_bleu(references, hypotheses, smoothing_function=smooth)\n",
    "print(f\"Corpus BLEU Score: {bleu_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fe6c83b-d683-4d1f-8862-37f509c73d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-14 20:55:35.002 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-14 20:55:35.727 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\Nishant\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-12-14 20:55:35.729 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-14 20:55:35.731 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-14 20:55:35.731 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-14 20:55:35.732 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-14 20:55:35.732 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-14 20:55:35.733 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-14 20:55:35.733 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-14 20:55:35.734 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-14 20:55:35.735 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-14 20:55:35.735 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import os\n",
    "\n",
    "# Set up the title and description of the app\n",
    "st.title(\"Upload Any File\")\n",
    "st.write(\"Please upload any file (CSV, TXT, DOCX, PDFs, Images, etc.).\")\n",
    "\n",
    "# Add file uploader widget for any type of file\n",
    "uploaded_file = st.file_uploader(\"Choose a file\", type=None)  # Accepts any file type\n",
    "\n",
    "# Check if a file has been uploaded\n",
    "if uploaded_file is not None:\n",
    "    # Get the file name and extension\n",
    "    file_name = uploaded_file.name\n",
    "    file_extension = os.path.splitext(file_name)[1].lower()\n",
    "    \n",
    "    # Display file details\n",
    "    st.write(f\"File Name: {file_name}\")\n",
    "    st.write(f\"File Extension: {file_extension}\")\n",
    "    \n",
    "    # Handling CSV file separately for processing\n",
    "    if file_extension == '.csv':\n",
    "        # If the file is a CSV, read it into a pandas DataFrame\n",
    "        dataset = pd.read_csv(uploaded_file)\n",
    "        st.write(\"Uploaded CSV Data:\")\n",
    "        st.write(dataset.head())\n",
    "\n",
    "    # Handling TXT file\n",
    "    elif file_extension == '.txt':\n",
    "        # If it's a text file, read the content and display it\n",
    "        file_content = uploaded_file.getvalue().decode(\"utf-8\")\n",
    "        st.write(\"Uploaded TXT File Content:\")\n",
    "        st.text(file_content)\n",
    "\n",
    "    # Handling other file types (e.g., DOCX, PDF, images)\n",
    "    elif file_extension in ['.docx', '.pdf']:\n",
    "        st.write(f\"Sorry, we do not support processing {file_extension} files yet.\")\n",
    "        \n",
    "    else:\n",
    "        # For other file types, simply display the file type and size\n",
    "        st.write(f\"Uploaded {file_extension} file, file size: {uploaded_file.size} bytes.\")\n",
    "        st.write(\"File content preview is not available for this type.\")\n",
    "    \n",
    "    # Optionally, allow the user to download the uploaded file\n",
    "    st.download_button(\n",
    "        label=\"Download Uploaded File\",\n",
    "        data=uploaded_file,\n",
    "        file_name=file_name,\n",
    "        mime=\"application/octet-stream\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d1143e6-5c20-4fd3-8dd8-ef7c5c1dc319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nishant\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3310e0b-e57f-4402-a7c6-b3ccd25ef2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Hello, how are you?\n",
      "Translated: Bonjour, comment allez-vous ?\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load the pre-trained MarianMT model for English to French translation\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-fr'  # Example: English to French translation\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Example sentence in English\n",
    "sentence = \"Hello, how are you?\"\n",
    "\n",
    "# Tokenize the input sentence\n",
    "inputs = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "\n",
    "# Perform translation (generate the output in French)\n",
    "translated = model.generate(inputs, max_length=50)\n",
    "\n",
    "# Decode the output to get the translated text\n",
    "translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the translated sentence\n",
    "print(f\"Original: {sentence}\")\n",
    "print(f\"Translated: {translated_text}\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8366d84-805d-4a30-9cb8-16a27a9e0a89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa893259b25a44db844f83bc23b807de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nishant\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Nishant\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-es. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b6650bd073487aa6b9c86d0e69fd55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/312M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ad99a04d514283957919f4acbe1220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad4ee05e6ba4904b0bdce96e12654a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a62f9cec0e40e0bcc4006db480d4af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "322ff6539f6548e3994725852bed3284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/826k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dffa4bdf7bb3497e8fbb0a6b423c4a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.59M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1420d68f3414a089ae9ea46bd76fe15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/312M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0, Loss: 4.3054\n",
      "Epoch 1/3, Average Loss: 3.4114\n",
      "Epoch 2, Batch 0, Loss: 2.1993\n",
      "Epoch 2/3, Average Loss: 1.7297\n",
      "Epoch 3, Batch 0, Loss: 1.1231\n",
      "Epoch 3/3, Average Loss: 0.8645\n",
      "Training time: 27.83 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "\n",
    "# Example data (replace with actual data)\n",
    "X_train = [\"Hello, how are you?\"] * 100  # Example training data (list of sentences)\n",
    "y_train = [\"Hola, ¿cómo estás?\"] * 100   # Example target data (list of sentences)\n",
    "\n",
    "# Load MarianMT Model and Tokenizer\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-es'  # Example model (English to Spanish)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set device (use GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Prepare DataLoader (convert to tensors)\n",
    "X_train_encoded = [tokenizer.encode(text, padding=True, truncation=True) for text in X_train]\n",
    "y_train_encoded = [tokenizer.encode(text, padding=True, truncation=True) for text in y_train]\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_encoded)\n",
    "y_train_tensor = torch.tensor(y_train_encoded)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Set optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "# Start tracking time\n",
    "start_time = time.time()\n",
    "\n",
    "# Training loop (just for checking)\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (input_ids, labels) in enumerate(train_loader):\n",
    "        input_ids, labels = input_ids.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Print batch info\n",
    "        if batch_idx % 10 == 0:  # Print every 10th batch\n",
    "            print(f\"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Average loss for the epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# End tracking time\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Training time: {training_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2090ab7-9400-43e1-9320-186623eaec27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 64, Epoch 1, Batch 0, Loss: 4.2424\n",
      "Epoch 1/20, Average Loss: 3.9655\n",
      "Batch Size: 64, Epoch 2, Batch 0, Loss: 3.1393\n",
      "Epoch 2/20, Average Loss: 2.8662\n",
      "Batch Size: 64, Epoch 3, Batch 0, Loss: 2.1641\n",
      "Epoch 3/20, Average Loss: 2.0029\n",
      "Batch Size: 64, Epoch 4, Batch 0, Loss: 1.5683\n",
      "Epoch 4/20, Average Loss: 1.4387\n",
      "Batch Size: 64, Epoch 5, Batch 0, Loss: 1.0765\n",
      "Epoch 5/20, Average Loss: 0.9875\n",
      "Batch Size: 64, Epoch 6, Batch 0, Loss: 0.7712\n",
      "Epoch 6/20, Average Loss: 0.7138\n",
      "Batch Size: 64, Epoch 7, Batch 0, Loss: 0.5818\n",
      "Epoch 7/20, Average Loss: 0.5339\n",
      "Batch Size: 64, Epoch 8, Batch 0, Loss: 0.4324\n",
      "Epoch 8/20, Average Loss: 0.3967\n",
      "Batch Size: 64, Epoch 9, Batch 0, Loss: 0.3060\n",
      "Epoch 9/20, Average Loss: 0.2727\n",
      "Batch Size: 64, Epoch 10, Batch 0, Loss: 0.1892\n",
      "Epoch 10/20, Average Loss: 0.1684\n",
      "Batch Size: 64, Epoch 11, Batch 0, Loss: 0.1052\n",
      "Epoch 11/20, Average Loss: 0.0938\n",
      "Batch Size: 64, Epoch 12, Batch 0, Loss: 0.0628\n",
      "Epoch 12/20, Average Loss: 0.0645\n",
      "Batch Size: 64, Epoch 13, Batch 0, Loss: 0.0455\n",
      "Epoch 13/20, Average Loss: 0.0423\n",
      "Batch Size: 64, Epoch 14, Batch 0, Loss: 0.0374\n",
      "Epoch 14/20, Average Loss: 0.0350\n",
      "Batch Size: 64, Epoch 15, Batch 0, Loss: 0.0306\n",
      "Epoch 15/20, Average Loss: 0.0299\n",
      "Batch Size: 64, Epoch 16, Batch 0, Loss: 0.0283\n",
      "Epoch 16/20, Average Loss: 0.0268\n",
      "Batch Size: 64, Epoch 17, Batch 0, Loss: 0.0237\n",
      "Epoch 17/20, Average Loss: 0.0227\n",
      "Batch Size: 64, Epoch 18, Batch 0, Loss: 0.0209\n",
      "Epoch 18/20, Average Loss: 0.0208\n",
      "Batch Size: 64, Epoch 19, Batch 0, Loss: 0.0188\n",
      "Epoch 19/20, Average Loss: 0.0184\n",
      "Batch Size: 64, Epoch 20, Batch 0, Loss: 0.0172\n",
      "Epoch 20/20, Average Loss: 0.0170\n",
      "Training time: 124.39 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "\n",
    "# Example data (replace with actual data)\n",
    "X_train = [\"Hello, how are you?\"] * 100  # Example training data (list of sentences)\n",
    "y_train = [\"Hola, ¿cómo estás?\"] * 100   # Example target data (list of sentences)\n",
    "\n",
    "# Load MarianMT Model and Tokenizer\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-es'  # Example model (English to Spanish)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set device (use GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Prepare DataLoader (convert to tensors)\n",
    "X_train_encoded = [tokenizer.encode(text, padding=True, truncation=True) for text in X_train]\n",
    "y_train_encoded = [tokenizer.encode(text, padding=True, truncation=True) for text in y_train]\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_encoded)\n",
    "y_train_tensor = torch.tensor(y_train_encoded)\n",
    "\n",
    "batch_size = 64  # Change batch size to 64 or desired number\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Set optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "# Start tracking time\n",
    "start_time = time.time()\n",
    "\n",
    "# Training loop (just for checking)\n",
    "epochs = 20  # Set the number of epochs to 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (input_ids, labels) in enumerate(train_loader):\n",
    "        input_ids, labels = input_ids.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Print batch info\n",
    "        if batch_idx % 10 == 0:  # Print every 10th batch\n",
    "            print(f\"Batch Size: {batch_size}, Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Average loss for the epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# End tracking time\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Training time: {training_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f24660cb-be53-49d4-886a-75a36482bc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 1e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Example setup with MarianMT model\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-es'  # Example model (English to Spanish)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Set device (use GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Set optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Print optimizer details\n",
    "print(optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e9589f9-7519-4070-b634-a76c48a89fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Total Loss: 45.8199, Average Loss: 14.6624\n",
      "Epoch 2/3, Total Loss: 41.7106, Average Loss: 13.3474\n",
      "Epoch 3/3, Total Loss: 39.5434, Average Loss: 12.6539\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Example setup with MarianMT model\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-es'  # Example model (English to Spanish)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Set device (use GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Set optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Dummy dataset (replace these with your actual data)\n",
    "# X_train and y_train should be your tokenized input and target data\n",
    "# For demonstration, we'll use dummy tensors (replace with actual data)\n",
    "X_train = torch.randint(0, 1000, (100, 20))  # 100 samples, 20 tokens each (replace with actual input)\n",
    "y_train = torch.randint(0, 1000, (100, 20))  # 100 samples, 20 tokens each (replace with actual target)\n",
    "\n",
    "# Train for a few epochs\n",
    "epochs = 3\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0  # Initialize total loss for each epoch\n",
    "    \n",
    "    # Example batch iteration (replace with your actual data loader)\n",
    "    for batch_idx in range(0, len(X_train), batch_size):\n",
    "        # Get the current batch (this is just an example)\n",
    "        input_batch = X_train[batch_idx: batch_idx + batch_size]\n",
    "        target_batch = y_train[batch_idx: batch_idx + batch_size]\n",
    "\n",
    "        # Move batches to the correct device (GPU or CPU)\n",
    "        input_batch = input_batch.to(device)\n",
    "        target_batch = target_batch.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_batch, labels=target_batch)\n",
    "        loss = outputs.loss  # Loss is part of the output in MarianMTModel\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    average_loss = total_loss / len(X_train) * batch_size  # Adjust based on batch size\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Total Loss: {total_loss:.4f}, Average Loss: {average_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fc4dc4-e8e0-4af8-b05c-62a71f581c63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
