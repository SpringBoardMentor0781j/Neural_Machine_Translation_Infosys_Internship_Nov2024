{"cells":[{"cell_type":"code","execution_count":null,"id":"aTbjVoNKfvW6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2975,"status":"ok","timestamp":1734621398296,"user":{"displayName":"UTKARSH KHARAT","userId":"15647262042698647088"},"user_tz":-330},"id":"aTbjVoNKfvW6","outputId":"2203064b-34e2-442e-9d94-37a0dd83a7b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"id":"sV7gHRYDfyMh","metadata":{"id":"sV7gHRYDfyMh"},"outputs":[],"source":["path = \"drive/My Drive/Project/EngToFrench.csv\""]},{"cell_type":"code","execution_count":null,"id":"b43eccbd-0521-4fb1-b65d-86336f80de09","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41533,"status":"ok","timestamp":1734621439827,"user":{"displayName":"UTKARSH KHARAT","userId":"15647262042698647088"},"user_tz":-330},"id":"b43eccbd-0521-4fb1-b65d-86336f80de09","outputId":"11e88dd0-4832-4bf1-ec09-a8087631869e","scrolled":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.11)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.5.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.15.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.10.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.5.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n","Collecting en-core-web-sm==3.7.1\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.11)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.1)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.12.14)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n","Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n","Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.67.1)\n","Collecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.17.0)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=462dce24f8648b6e95f9462bd4460b2ace0851484bed0c1aa2b8f11404479515\n","  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n","Successfully built langdetect\n","Installing collected packages: langdetect\n","Successfully installed langdetect-1.0.9\n"]}],"source":["!pip install nltk spacy\n","!python -m spacy download en_core_web_sm\n","!pip install pandas\n","!pip install textblob\n","!pip install langdetect"]},{"cell_type":"code","execution_count":null,"id":"60e660cd-5baf-4500-a703-a7ea23e086ca","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11848,"status":"ok","timestamp":1734621452308,"user":{"displayName":"UTKARSH KHARAT","userId":"15647262042698647088"},"user_tz":-330},"id":"60e660cd-5baf-4500-a703-a7ea23e086ca","outputId":"7f01e4ed-7ac6-488f-811b-4465d5a130a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["  English words/sentences French words/sentences\n","0                     Hi.                 Salut!\n","1                    Run!              Cours??!\n","2                    Run!             Courez??!\n","3                    Who?                  Qui ?\n","4                    Wow!          ??a alors??!\n"]}],"source":["import pandas as pd\n","import re\n","from langdetect import detect\n","from textblob import TextBlob\n","\n","# Load the data\n","file_path = 'EngToFrench.csv'  # Replace with your file path\n","data = pd.read_csv(path, encoding='latin-1')\n","\n","# Display first few rows to inspect\n","print(data.head())\n"]},{"cell_type":"code","execution_count":null,"id":"805bfb49-84ff-4908-8302-0939652ef682","metadata":{"id":"805bfb49-84ff-4908-8302-0939652ef682"},"outputs":[],"source":["def normalize_text(text):\n","    return text.lower().strip()  # Convert to lowercase and remove extra spaces\n"]},{"cell_type":"code","execution_count":null,"id":"5c622fca-c20e-4451-bd92-700250519fc2","metadata":{"id":"5c622fca-c20e-4451-bd92-700250519fc2"},"outputs":[],"source":["def simple_tokenize(text):\n","    return text.split()  # Tokenize by splitting on whitespace\n"]},{"cell_type":"code","execution_count":null,"id":"16a8ea5f-0a8e-42f0-8acd-289d82040ac0","metadata":{"id":"16a8ea5f-0a8e-42f0-8acd-289d82040ac0"},"outputs":[],"source":["def remove_special_characters(text):\n","    return re.sub(r'[^\\w\\s]', '', text)  # Remove special characters except spaces\n"]},{"cell_type":"code","execution_count":null,"id":"7f881f44-7b52-429a-8098-a20fe352bfbc","metadata":{"id":"7f881f44-7b52-429a-8098-a20fe352bfbc"},"outputs":[],"source":["def handle_numbers(text):\n","    return re.sub(r'\\d+', '<number>', text)  # Replace numbers with a placeholder\n"]},{"cell_type":"code","execution_count":null,"id":"bb498770-4bf2-433e-8594-dfb672cbb3c9","metadata":{"id":"bb498770-4bf2-433e-8594-dfb672cbb3c9"},"outputs":[],"source":["# Apply text normalization\n","data['English Normalized'] = data['English words/sentences'].apply(normalize_text)\n","data['French Normalized'] = data['French words/sentences'].apply(normalize_text)\n","\n","# Apply tokenization\n","data['English Tokenized'] = data['English Normalized'].apply(simple_tokenize)\n","data['French Tokenized'] = data['French Normalized'].apply(simple_tokenize)\n","\n","# Remove special characters\n","data['English Cleaned'] = data['English Normalized'].apply(remove_special_characters)\n","data['French Cleaned'] = data['French Normalized'].apply(remove_special_characters)\n","\n","# Handle numbers\n","data['English Numbers Handled'] = data['English Cleaned'].apply(handle_numbers)\n","data['French Numbers Handled'] = data['French Cleaned'].apply(handle_numbers)\n"]},{"cell_type":"code","execution_count":null,"id":"da4090ff-ab57-4a13-b31c-291553a17806","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3326,"status":"ok","timestamp":1734621458532,"user":{"displayName":"UTKARSH KHARAT","userId":"15647262042698647088"},"user_tz":-330},"id":"da4090ff-ab57-4a13-b31c-291553a17806","outputId":"37a0aafb-f38c-4567-d1f4-aa7027df9f94"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenizers loaded successfully.\n","Tokenizers saved successfully.\n","  English words/sentences French words/sentences English Normalized  \\\n","0                     Hi.                 Salut!                hi.   \n","1                    Run!              Cours??!               run!   \n","2                    Run!             Courez??!               run!   \n","3                    Who?                  Qui ?               who?   \n","4                    Wow!          ??a alors??!               wow!   \n","\n","  French Normalized English Tokenized  French Tokenized English Cleaned  \\\n","0            salut!             [hi.]          [salut!]              hi   \n","1         cours??!            [run!]       [cours??!]             run   \n","2        courez??!            [run!]      [courez??!]             run   \n","3             qui ?            [who?]          [qui, ?]             who   \n","4     ??a alors??!            [wow!]  [??a, alors??!]             wow   \n","\n","  French Cleaned English Numbers Handled French Numbers Handled  \n","0          salut                      hi                  salut  \n","1          cours                     run                  cours  \n","2         courez                     run                 courez  \n","3           qui                      who                   qui   \n","4        a alors                     wow                a alors  \n"]}],"source":["import pickle\n","import pandas as pd\n","from tensorflow.keras.preprocessing.text import Tokenizer  # Updated import\n","\n","# Initialize Tokenizers if they are not already defined\n","try:\n","    # Load existing tokenizers\n","    with open('/content/drive/MyDrive/Project/eng_tokenizer.pkl', 'rb') as f:\n","        eng_tokenizer = pickle.load(f)\n","    with open('/content/drive/MyDrive/Project/fr_tokenizer.pkl', 'rb') as f:\n","        fr_tokenizer = pickle.load(f)\n","    print(\"Tokenizers loaded successfully.\")\n","except (FileNotFoundError, EOFError):\n","    print(\"Tokenizer files not found or corrupted. Initializing new tokenizers.\")\n","    eng_tokenizer = Tokenizer()\n","    fr_tokenizer = Tokenizer()\n","    # Add tokenizer fitting logic if required, e.g.:\n","    # eng_tokenizer.fit_on_texts(english_sentences)\n","    # fr_tokenizer.fit_on_texts(french_sentences)\n","\n","# Save Tokenizers\n","try:\n","    with open('/content/drive/MyDrive/Project/eng_tokenizer.pkl', 'wb') as f:\n","        pickle.dump(eng_tokenizer, f)\n","    with open('/content/drive/MyDrive/Project/fr_tokenizer.pkl', 'wb') as f:\n","        pickle.dump(fr_tokenizer, f)\n","    print(\"Tokenizers saved successfully.\")\n","except Exception as e:\n","    print(f\"Error saving tokenizers: {e}\")\n","\n","# Ensure `data` is preprocessed and defined\n","try:\n","    # Simulate preprocessed data creation if `data` is undefined\n","    if 'data' not in locals():\n","        # Example of creating dummy data (replace with your actual preprocessing logic)\n","        data = pd.DataFrame({\n","            \"English\": [\"Hello\", \"How are you?\", \"Goodbye\"],\n","            \"French\": [\"Bonjour\", \"Comment ça va?\", \"Au revoir\"]\n","        })\n","        print(\"Dummy data created for demonstration.\")\n","\n","    # Save the preprocessed data to a new file\n","    data.to_csv('/content/drive/MyDrive/Project/Preprocessed_EngToFrench.csv', index=False)\n","\n","    # Print a sample of the preprocessed data\n","    print(data.head())\n","except NameError as e:\n","    print(f\"Error: {e}. Make sure 'data' is defined and contains your preprocessed data.\")\n","except Exception as e:\n","    print(f\"Error with data processing: {e}\")\n","\n"]},{"cell_type":"code","execution_count":null,"id":"0985c1c2-3216-4c6a-bf6b-1ff7b1ad6d23","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6180,"status":"ok","timestamp":1734621464709,"user":{"displayName":"UTKARSH KHARAT","userId":"15647262042698647088"},"user_tz":-330},"id":"0985c1c2-3216-4c6a-bf6b-1ff7b1ad6d23","outputId":"6baf12fe-e16e-49f7-c272-ce42e8ec2828"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/brown.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/conll2000.zip.\n","[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/movie_reviews.zip.\n","Finished.\n"]}],"source":["!python -m textblob.download_corpora\n"]},{"cell_type":"code","execution_count":null,"id":"658ac729-4045-4bd2-8e92-fed6c2527c8c","metadata":{"id":"658ac729-4045-4bd2-8e92-fed6c2527c8c"},"outputs":[],"source":["def remove_duplicates(df):\n","    # If your column contains lists, you can convert them into strings\n","    for col in df.columns:\n","        df[col] = df[col].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n","\n","    return df.drop_duplicates()"]},{"cell_type":"code","execution_count":null,"id":"a138819d-834c-485b-a083-9f18d24b393e","metadata":{"id":"a138819d-834c-485b-a083-9f18d24b393e"},"outputs":[],"source":["def detect_language(text):\n","    try:\n","        return detect(text)\n","    except:\n","        return None\n"]},{"cell_type":"code","execution_count":null,"id":"82f61117-37d1-437d-a1f7-e2012f3cf8b0","metadata":{"id":"82f61117-37d1-437d-a1f7-e2012f3cf8b0"},"outputs":[],"source":["def filter_sentence_length(text, min_len=2, max_len=100):\n","    words = text.split()\n","    return min_len <= len(words) <= max_len\n"]},{"cell_type":"code","execution_count":null,"id":"90c7387d-117b-4cf7-bb3b-175c2d959a91","metadata":{"id":"90c7387d-117b-4cf7-bb3b-175c2d959a91"},"outputs":[],"source":["def correct_spelling(text):\n","    blob = TextBlob(text)\n","    return str(blob.correct())\n"]},{"cell_type":"code","execution_count":null,"id":"f5f7c453-8c4f-47b3-9235-67f5d9dc8791","metadata":{"id":"f5f7c453-8c4f-47b3-9235-67f5d9dc8791"},"outputs":[],"source":["def check_consistency(row):\n","    # Ensure both sentences have similar lengths\n","    eng_len = len(row['English words/sentences'].split())\n","    fr_len = len(row['French words/sentences'].split())\n","    return abs(eng_len - fr_len) <= 3\n"]},{"cell_type":"code","execution_count":null,"id":"a5c4b52f-c72f-4922-9e02-a9f4606cd8df","metadata":{"id":"a5c4b52f-c72f-4922-9e02-a9f4606cd8df"},"outputs":[],"source":["def remove_noise(text):\n","    # Example: Remove placeholder tokens or extra whitespace\n","    return text.replace('<number>', '').strip()\n"]},{"cell_type":"code","execution_count":null,"id":"ba8f7306-c933-4117-9460-bc78bf11c4f5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1585193,"status":"ok","timestamp":1734623050339,"user":{"displayName":"UTKARSH KHARAT","userId":"15647262042698647088"},"user_tz":-330},"id":"ba8f7306-c933-4117-9460-bc78bf11c4f5","outputId":"1aa25720-6c1f-4525-e201-c9931016ca27"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-29-78fa7fd4cd12>:43: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  data['English Cleaned'] = data['English Corrected'].apply(remove_noise)\n"]}],"source":["# 1. Remove duplicates\n","data = remove_duplicates(data)\n","\n","\n","# Convert lists back to strings if needed\n","data['English words/sentences'] = data['English words/sentences'].apply(\n","    lambda x: ' '.join(x) if isinstance(x, list) else x\n",")\n","data['French words/sentences'] = data['French words/sentences'].apply(\n","    lambda x: ' '.join(x) if isinstance(x, list) else x\n",")\n","\n","\n","# Apply cleaning functions\n","data['English Normalized'] = data['English words/sentences'].apply(normalize_text)\n","data['French Normalized'] = data['French words/sentences'].apply(normalize_text)\n","data['French words/sentences'] = data['French words/sentences'].apply(\n","    lambda x: ' '.join(x) if isinstance(x, list) else x\n",")\n","\n","# 2. Detect language\n","data['English Language'] = data['English words/sentences'].apply(detect_language)\n","data['French Language'] = data['French words/sentences'].apply(detect_language)\n","\n","# Filter out rows where language detection fails\n","data = data[(data['English Language'] == 'en') & (data['French Language'] == 'fr')]\n","\n","# 3. Filter short/long sentences\n","data = data[data['English words/sentences'].apply(lambda x: filter_sentence_length(x))]\n","data = data[data['French words/sentences'].apply(lambda x: filter_sentence_length(x))]\n","\n","# 4. Correct spelling errors\n","data['English Corrected'] = data['English words/sentences'].apply(correct_spelling)\n","data['French Corrected'] = data['French words/sentences']  # Skip for non-English\n","\n","# 5. Contextual relevance (skipped as it requires domain knowledge)\n","\n","# 6. Check consistency\n","data['Consistent'] = data.apply(check_consistency, axis=1)\n","data = data[data['Consistent']]\n","\n","# 7. Remove noise\n","data['English Cleaned'] = data['English Corrected'].apply(remove_noise)\n","data['French Cleaned'] = data['French Corrected'].apply(remove_noise)\n","\n","# 8. Quality control (skipped as it depends on specific requirements)\n"]},{"cell_type":"code","execution_count":null,"id":"b1f47839-5777-405b-b4d9-5e9e9bf0080a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":422,"status":"ok","timestamp":1734623050758,"user":{"displayName":"UTKARSH KHARAT","userId":"15647262042698647088"},"user_tz":-330},"id":"b1f47839-5777-405b-b4d9-5e9e9bf0080a","outputId":"3c5c2f3e-830f-4d21-846c-7b24772762be"},"outputs":[{"output_type":"stream","name":"stdout","text":["Preprocessed data saved to /content/drive/MyDrive/Project/Preprocessed_EngToFrench.csv\n","    English words/sentences French words/sentences English Normalized  \\\n","78                 Be cool.        Sois d??tendu !           be cool.   \n","99                 Come in.                Entre !           come in.   \n","125                Go home.  Rentrez ?ÿ la maison.           go home.   \n","126                Go home.   Rentre ?ÿ la maison.           go home.   \n","146                Hold on.        Ne quittez pas.           hold on.   \n","\n","         French Normalized English Tokenized       French Tokenized  \\\n","78         sois d??tendu !          be cool.        sois d??tendu !   \n","99                 entre !          come in.                entre !   \n","125  rentrez ?ÿ la maison.          go home.  rentrez ?ÿ la maison.   \n","126   rentre ?ÿ la maison.          go home.   rentre ?ÿ la maison.   \n","146        ne quittez pas.          hold on.        ne quittez pas.   \n","\n","    English Cleaned         French Cleaned English Numbers Handled  \\\n","78         He cool.        Sois d??tendu !                 be cool   \n","99         Some in.                Entre !                 come in   \n","125        To home.  Rentrez ?ÿ la maison.                 go home   \n","126        To home.   Rentre ?ÿ la maison.                 go home   \n","146         Old on.        Ne quittez pas.                 hold on   \n","\n","    French Numbers Handled English Language French Language English Corrected  \\\n","78            sois dtendu                en              fr          He cool.   \n","99                  entre                en              fr          Some in.   \n","125    rentrez ÿ la maison               en              fr          To home.   \n","126     rentre ÿ la maison               en              fr          To home.   \n","146         ne quittez pas               en              fr           Old on.   \n","\n","          French Corrected  Consistent  \n","78         Sois d??tendu !        True  \n","99                 Entre !        True  \n","125  Rentrez ?ÿ la maison.        True  \n","126   Rentre ?ÿ la maison.        True  \n","146        Ne quittez pas.        True  \n"]}],"source":["# Save the cleaned data\n","#data.to_csv('Cleaned_EngToFrench.csv', index=False)\n","# Save preprocessed data\n","\n","#data.to_csv('/content/drive/MyDrive/Project/Preprocessed_EngToFrench.csv', index=False)\n","preprocessed_path = '/content/drive/MyDrive/Project/Preprocessed_EngToFrench.csv'\n","data.to_csv(preprocessed_path, index=False)\n","print(f\"Preprocessed data saved to {preprocessed_path}\")\n","\n","# Display a sample of the cleaned data\n","print(data.head())\n"]},{"cell_type":"code","execution_count":null,"id":"bYBaC_SVSAZe","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":988,"status":"ok","timestamp":1734623051744,"user":{"displayName":"UTKARSH KHARAT","userId":"15647262042698647088"},"user_tz":-330},"id":"bYBaC_SVSAZe","outputId":"a4b36668-b077-42bd-d551-6e90bc58d3bb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Preprocessed data saved to /content/drive/MyDrive/Project/Preprocessed_EngToFrench.csv\n","Preprocessed data loaded successfully.\n"]}],"source":["# Save Preprocessed Data\n","preprocessed_path = '/content/drive/MyDrive/Project/Preprocessed_EngToFrench.csv'\n","data.to_csv(preprocessed_path, index=False)\n","print(f\"Preprocessed data saved to {preprocessed_path}\")\n","\n","#Load Data\n","preprocessed_path = '/content/drive/MyDrive/Project/Preprocessed_EngToFrench.csv'\n","data = pd.read_csv(preprocessed_path)\n","print(\"Preprocessed data loaded successfully.\")\n"]},{"cell_type":"code","execution_count":null,"id":"W2t-HkNZSxTN","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22963,"status":"ok","timestamp":1734623075353,"user":{"displayName":"UTKARSH KHARAT","userId":"15647262042698647088"},"user_tz":-330},"id":"W2t-HkNZSxTN","outputId":"147c0a06-7473-40e5-98fc-4ef73547e34b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n","Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n","Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (1.0.9)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.11)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.5.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.15.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.10.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.5.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.17.0)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n","Collecting en-core-web-sm==3.7.1\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.11)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.1)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.12.14)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n","Environment setup completed.\n"]}],"source":["# Install necessary libraries\n","!pip install nltk spacy pandas textblob langdetect\n","!python -m spacy download en_core_web_sm\n","print(\"Environment setup completed.\")\n","\n","# Mount Google Drive\n","#from google.colab import drive\n","#drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"id":"d6972d01-d5d1-4649-afb1-ed961514f01d","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":660},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1734623075354,"user":{"displayName":"UTKARSH KHARAT","userId":"15647262042698647088"},"user_tz":-330},"id":"d6972d01-d5d1-4649-afb1-ed961514f01d","outputId":"683e5a47-5b92-4dbe-e892-2b357a7622a2","scrolled":true},"outputs":[{"output_type":"execute_result","data":{"text/plain":["         English words/sentences  \\\n","38095  Give me a coffee, please.   \n","38096  Give me a coffee, please.   \n","38097  Give me a glass of water.   \n","38098  Give me a hand with this.   \n","38099  Give me a hand with this.   \n","\n","                             French words/sentences  \\\n","38095         Donnez-moi un caf??, s'il vous plait.   \n","38096            Donnez-moi un caf??, je vous prie.   \n","38097  Donnez-moi un verre d'eau, s'il vous pla??t.   \n","38098           Donne-moi un coup de main pour ??a.   \n","38099         Donnez-moi un coup de main pour ceci.   \n","\n","              English Normalized  \\\n","38095  give me a coffee, please.   \n","38096  give me a coffee, please.   \n","38097  give me a glass of water.   \n","38098  give me a hand with this.   \n","38099  give me a hand with this.   \n","\n","                                  French Normalized  \\\n","38095         donnez-moi un caf??, s'il vous plait.   \n","38096            donnez-moi un caf??, je vous prie.   \n","38097  donnez-moi un verre d'eau, s'il vous pla??t.   \n","38098           donne-moi un coup de main pour ??a.   \n","38099         donnez-moi un coup de main pour ceci.   \n","\n","               English Tokenized  \\\n","38095  give me a coffee, please.   \n","38096  give me a coffee, please.   \n","38097  give me a glass of water.   \n","38098  give me a hand with this.   \n","38099  give me a hand with this.   \n","\n","                                   French Tokenized  \\\n","38095         donnez-moi un caf??, s'il vous plait.   \n","38096            donnez-moi un caf??, je vous prie.   \n","38097  donnez-moi un verre d'eau, s'il vous pla??t.   \n","38098           donne-moi un coup de main pour ??a.   \n","38099         donnez-moi un coup de main pour ceci.   \n","\n","                 English Cleaned  \\\n","38095  Give me a coffee, please.   \n","38096  Give me a coffee, please.   \n","38097  Give me a glass of water.   \n","38098  Give me a hand with this.   \n","38099  Give me a hand with this.   \n","\n","                                     French Cleaned   English Numbers Handled  \\\n","38095         Donnez-moi un caf??, s'il vous plait.   give me a coffee please   \n","38096            Donnez-moi un caf??, je vous prie.   give me a coffee please   \n","38097  Donnez-moi un verre d'eau, s'il vous pla??t.  give me a glass of water   \n","38098           Donne-moi un coup de main pour ??a.  give me a hand with this   \n","38099         Donnez-moi un coup de main pour ceci.  give me a hand with this   \n","\n","                      French Numbers Handled English Language French Language  \\\n","38095        donnezmoi un caf sil vous plait               en              fr   \n","38096          donnezmoi un caf je vous prie               en              fr   \n","38097  donnezmoi un verre deau sil vous plat               en              fr   \n","38098        donnemoi un coup de main pour a               en              fr   \n","38099    donnezmoi un coup de main pour ceci               en              fr   \n","\n","               English Corrected  \\\n","38095  Give me a coffee, please.   \n","38096  Give me a coffee, please.   \n","38097  Give me a glass of water.   \n","38098  Give me a hand with this.   \n","38099  Give me a hand with this.   \n","\n","                                   French Corrected  Consistent  \n","38095         Donnez-moi un caf??, s'il vous plait.        True  \n","38096            Donnez-moi un caf??, je vous prie.        True  \n","38097  Donnez-moi un verre d'eau, s'il vous pla??t.        True  \n","38098           Donne-moi un coup de main pour ??a.        True  \n","38099         Donnez-moi un coup de main pour ceci.        True  "],"text/html":["\n","  <div id=\"df-f25fe9c1-fe06-483e-81c4-9248c5b97d36\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>English words/sentences</th>\n","      <th>French words/sentences</th>\n","      <th>English Normalized</th>\n","      <th>French Normalized</th>\n","      <th>English Tokenized</th>\n","      <th>French Tokenized</th>\n","      <th>English Cleaned</th>\n","      <th>French Cleaned</th>\n","      <th>English Numbers Handled</th>\n","      <th>French Numbers Handled</th>\n","      <th>English Language</th>\n","      <th>French Language</th>\n","      <th>English Corrected</th>\n","      <th>French Corrected</th>\n","      <th>Consistent</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>38095</th>\n","      <td>Give me a coffee, please.</td>\n","      <td>Donnez-moi un caf??, s'il vous plait.</td>\n","      <td>give me a coffee, please.</td>\n","      <td>donnez-moi un caf??, s'il vous plait.</td>\n","      <td>give me a coffee, please.</td>\n","      <td>donnez-moi un caf??, s'il vous plait.</td>\n","      <td>Give me a coffee, please.</td>\n","      <td>Donnez-moi un caf??, s'il vous plait.</td>\n","      <td>give me a coffee please</td>\n","      <td>donnezmoi un caf sil vous plait</td>\n","      <td>en</td>\n","      <td>fr</td>\n","      <td>Give me a coffee, please.</td>\n","      <td>Donnez-moi un caf??, s'il vous plait.</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>38096</th>\n","      <td>Give me a coffee, please.</td>\n","      <td>Donnez-moi un caf??, je vous prie.</td>\n","      <td>give me a coffee, please.</td>\n","      <td>donnez-moi un caf??, je vous prie.</td>\n","      <td>give me a coffee, please.</td>\n","      <td>donnez-moi un caf??, je vous prie.</td>\n","      <td>Give me a coffee, please.</td>\n","      <td>Donnez-moi un caf??, je vous prie.</td>\n","      <td>give me a coffee please</td>\n","      <td>donnezmoi un caf je vous prie</td>\n","      <td>en</td>\n","      <td>fr</td>\n","      <td>Give me a coffee, please.</td>\n","      <td>Donnez-moi un caf??, je vous prie.</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>38097</th>\n","      <td>Give me a glass of water.</td>\n","      <td>Donnez-moi un verre d'eau, s'il vous pla??t.</td>\n","      <td>give me a glass of water.</td>\n","      <td>donnez-moi un verre d'eau, s'il vous pla??t.</td>\n","      <td>give me a glass of water.</td>\n","      <td>donnez-moi un verre d'eau, s'il vous pla??t.</td>\n","      <td>Give me a glass of water.</td>\n","      <td>Donnez-moi un verre d'eau, s'il vous pla??t.</td>\n","      <td>give me a glass of water</td>\n","      <td>donnezmoi un verre deau sil vous plat</td>\n","      <td>en</td>\n","      <td>fr</td>\n","      <td>Give me a glass of water.</td>\n","      <td>Donnez-moi un verre d'eau, s'il vous pla??t.</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>38098</th>\n","      <td>Give me a hand with this.</td>\n","      <td>Donne-moi un coup de main pour ??a.</td>\n","      <td>give me a hand with this.</td>\n","      <td>donne-moi un coup de main pour ??a.</td>\n","      <td>give me a hand with this.</td>\n","      <td>donne-moi un coup de main pour ??a.</td>\n","      <td>Give me a hand with this.</td>\n","      <td>Donne-moi un coup de main pour ??a.</td>\n","      <td>give me a hand with this</td>\n","      <td>donnemoi un coup de main pour a</td>\n","      <td>en</td>\n","      <td>fr</td>\n","      <td>Give me a hand with this.</td>\n","      <td>Donne-moi un coup de main pour ??a.</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>38099</th>\n","      <td>Give me a hand with this.</td>\n","      <td>Donnez-moi un coup de main pour ceci.</td>\n","      <td>give me a hand with this.</td>\n","      <td>donnez-moi un coup de main pour ceci.</td>\n","      <td>give me a hand with this.</td>\n","      <td>donnez-moi un coup de main pour ceci.</td>\n","      <td>Give me a hand with this.</td>\n","      <td>Donnez-moi un coup de main pour ceci.</td>\n","      <td>give me a hand with this</td>\n","      <td>donnezmoi un coup de main pour ceci</td>\n","      <td>en</td>\n","      <td>fr</td>\n","      <td>Give me a hand with this.</td>\n","      <td>Donnez-moi un coup de main pour ceci.</td>\n","      <td>True</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f25fe9c1-fe06-483e-81c4-9248c5b97d36')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-f25fe9c1-fe06-483e-81c4-9248c5b97d36 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-f25fe9c1-fe06-483e-81c4-9248c5b97d36');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-4b7cc312-b210-4ce2-b75c-c2cad99e4812\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4b7cc312-b210-4ce2-b75c-c2cad99e4812')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-4b7cc312-b210-4ce2-b75c-c2cad99e4812 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"data\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"English words/sentences\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Give me a coffee, please.\",\n          \"Give me a glass of water.\",\n          \"Give me a hand with this.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"French words/sentences\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Donnez-moi un caf??, je vous prie.\",\n          \"Donnez-moi un coup de main pour ceci.\",\n          \"Donnez-moi un verre d'eau, s'il vous pla??t.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"English Normalized\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"give me a coffee, please.\",\n          \"give me a glass of water.\",\n          \"give me a hand with this.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"French Normalized\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"donnez-moi un caf??, je vous prie.\",\n          \"donnez-moi un coup de main pour ceci.\",\n          \"donnez-moi un verre d'eau, s'il vous pla??t.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"English Tokenized\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"give me a coffee, please.\",\n          \"give me a glass of water.\",\n          \"give me a hand with this.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"French Tokenized\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"donnez-moi un caf??, je vous prie.\",\n          \"donnez-moi un coup de main pour ceci.\",\n          \"donnez-moi un verre d'eau, s'il vous pla??t.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"English Cleaned\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Give me a coffee, please.\",\n          \"Give me a glass of water.\",\n          \"Give me a hand with this.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"French Cleaned\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Donnez-moi un caf??, je vous prie.\",\n          \"Donnez-moi un coup de main pour ceci.\",\n          \"Donnez-moi un verre d'eau, s'il vous pla??t.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"English Numbers Handled\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"give me a coffee please\",\n          \"give me a glass of water\",\n          \"give me a hand with this\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"French Numbers Handled\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"donnezmoi un caf je vous prie\",\n          \"donnezmoi un coup de main pour ceci\",\n          \"donnezmoi un verre deau sil vous plat\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"English Language\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"en\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"French Language\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"fr\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"English Corrected\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Give me a coffee, please.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"French Corrected\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Donnez-moi un caf??, je vous prie.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Consistent\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":33}],"source":["data.tail()"]},{"cell_type":"code","execution_count":null,"id":"plc36JUWMqsz","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8451,"status":"ok","timestamp":1734623083798,"user":{"displayName":"UTKARSH KHARAT","userId":"15647262042698647088"},"user_tz":-330},"id":"plc36JUWMqsz","outputId":"bc5c1d20-2543-4a80-cfb1-8fda1f681ede"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","#Prepare Data for traning\n","\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","import pandas as pd\n","from tensorflow.keras.preprocessing.text import Tokenizer  # Import Tokenizer\n","\n","# Load the preprocessed data\n","#data = pd.read_csv('/content/drive/MyDrive/Project/Preprocessed_EngToFrench.csv')\n","data = pd.read_csv('/content/drive/MyDrive/Project/Preprocessed_EngToFrench.csv', encoding='latin-1')\n","\n","# Initialize the tokenizers (this is the crucial part)\n","eng_tokenizer = Tokenizer()\n","fr_tokenizer = Tokenizer()\n","\n","# Tokenize English and French text\n","eng_tokenizer.fit_on_texts(data['English Cleaned'])\n","fr_tokenizer.fit_on_texts(data['French Cleaned'])\n","\n","# Convert text to sequences\n","eng_sequences = eng_tokenizer.texts_to_sequences(data['English Cleaned'])\n","fr_sequences = fr_tokenizer.texts_to_sequences(data['French Cleaned'])\n","\n","# Padding the sequences\n","max_len = 20  # Maximum length of sequences (adjust as needed)\n","X_data = pad_sequences(eng_sequences, maxlen=max_len, padding='post')\n","y_data = pad_sequences(fr_sequences, maxlen=max_len, padding='post')\n","\n","# Split into training and testing sets (70% for training, 30% for testing)\n","X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.3, random_state=42)\n","\n","# Model parameters\n","latent_dim = 256  # Latent dimension size\n","vocab_size = len(eng_tokenizer.word_index) + 1  # Encoder vocab size (English)\n","target_vocab_size = len(fr_tokenizer.word_index) + 1  # Decoder vocab size (French)"]},{"cell_type":"code","execution_count":null,"id":"XeMvYOI8npZq","metadata":{"id":"XeMvYOI8npZq"},"outputs":[],"source":["#Build the NMT model.\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n","from tensorflow.keras.optimizers import Adam\n","\n","# Encoder\n","encoder_inputs = Input(shape=(None,), name='encoder_inputs')\n","encoder_embedding = Embedding(input_dim=vocab_size, output_dim=latent_dim)(encoder_inputs)\n","encoder_lstm = LSTM(latent_dim, return_state=True)\n","encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n","\n","# Decoder\n","decoder_inputs = Input(shape=(None,), name='decoder_inputs')\n","decoder_embedding = Embedding(input_dim=target_vocab_size, output_dim=latent_dim)(decoder_inputs)\n","decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])\n","decoder_dense = Dense(target_vocab_size, activation='softmax')\n","decoder_outputs_final = decoder_dense(decoder_outputs)\n","\n","# Build the full model\n","nmt_model = Model([encoder_inputs, decoder_inputs], decoder_outputs_final)\n","\n","# Compile the model with Adam optimizer and sparse categorical crossentropy loss\n","optimizer = Adam(clipvalue=1.0)  # Gradient clipping\n","nmt_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"]},{"cell_type":"code","execution_count":null,"id":"gkuaYpJ5n3A0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gkuaYpJ5n3A0","outputId":"62e6c255-02c2-4b54-e8e1-ad993e9e1ea7","executionInfo":{"status":"ok","timestamp":1734632556159,"user_tz":-330,"elapsed":9471739,"user":{"displayName":"UTKARSH KHARAT","userId":"15647262042698647088"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m919s\u001b[0m 3s/step - accuracy: 0.7818 - loss: 2.5242 - val_accuracy: 0.8031 - val_loss: 1.3495\n","Epoch 2/10\n","\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m913s\u001b[0m 3s/step - accuracy: 0.8063 - loss: 1.3015 - val_accuracy: 0.8128 - val_loss: 1.2545\n","Epoch 3/10\n","\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m938s\u001b[0m 3s/step - accuracy: 0.8168 - loss: 1.1911 - val_accuracy: 0.8229 - val_loss: 1.1615\n","Epoch 4/10\n","\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m926s\u001b[0m 3s/step - accuracy: 0.8268 - loss: 1.0811 - val_accuracy: 0.8328 - val_loss: 1.0748\n","Epoch 5/10\n","\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m948s\u001b[0m 3s/step - accuracy: 0.8395 - loss: 0.9636 - val_accuracy: 0.8424 - val_loss: 0.9948\n","Epoch 6/10\n","\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m972s\u001b[0m 3s/step - accuracy: 0.8495 - loss: 0.8646 - val_accuracy: 0.8501 - val_loss: 0.9341\n","Epoch 7/10\n","\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m968s\u001b[0m 3s/step - accuracy: 0.8589 - loss: 0.7732 - val_accuracy: 0.8552 - val_loss: 0.8853\n","Epoch 8/10\n","\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m941s\u001b[0m 3s/step - accuracy: 0.8676 - loss: 0.6964 - val_accuracy: 0.8618 - val_loss: 0.8411\n","Epoch 9/10\n","\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m949s\u001b[0m 3s/step - accuracy: 0.8765 - loss: 0.6233 - val_accuracy: 0.8661 - val_loss: 0.8092\n","Epoch 10/10\n","\u001b[1m334/334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m942s\u001b[0m 3s/step - accuracy: 0.8855 - loss: 0.5580 - val_accuracy: 0.8698 - val_loss: 0.7820\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x79206790ee30>"]},"metadata":{},"execution_count":36}],"source":["#Prepare Decoding data for Traning\n","# Prepare decoder input and target output (shifted) with padding\n","import tensorflow as tf # Import TensorFlow\n","\n","decoder_input_data = pad_sequences(y_train[:, :-1], maxlen=max_len, padding='post')\n","decoder_target_data = pad_sequences(y_train[:, 1:], maxlen=max_len, padding='post')\n","\n","# Train the model\n","nmt_model.fit(\n","    [X_train, decoder_input_data],\n","    np.expand_dims(decoder_target_data, -1),  # Add an extra dimension for the target labels\n","    epochs=10,  # You can adjust the epochs\n","    batch_size=64,\n","    validation_split=0.2,\n","    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)]  # Stop early if no improvement\n",")"]},{"cell_type":"code","execution_count":null,"id":"NYOBi5-9rLQM","metadata":{"id":"NYOBi5-9rLQM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734632556159,"user_tz":-330,"elapsed":5,"user":{"displayName":"UTKARSH KHARAT","userId":"15647262042698647088"}},"outputId":"579c43c8-2018-496e-d927-3d9aa7e1af14"},"outputs":[{"output_type":"stream","name":"stdout","text":["Preparing test decoder input and target output...\n","Test data prepared successfully!\n"]}],"source":["# Prepare test decoder input and target output\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","print(\"Preparing test decoder input and target output...\")\n","\n","decoder_input_test = pad_sequences(y_test[:, :-1], maxlen=max_len, padding='post')  # Decoder inputs\n","decoder_target_test = pad_sequences(y_test[:, 1:], maxlen=max_len, padding='post')  # Decoder outputs\n","\n","print(\"Test data prepared successfully!\")\n"]},{"cell_type":"code","source":["# Evaluate the model on the test set\n","print(\"Evaluating the model on the test data...\")\n","\n","loss, accuracy = nmt_model.evaluate(\n","    [X_test, decoder_input_test],\n","    np.expand_dims(decoder_target_test, -1),  # Expand dimension for compatibility\n","    batch_size=64\n",")\n","\n","print(f\"Test Loss: {loss:.4f}\")\n","print(f\"Test Accuracy: {accuracy:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cychSa2PO5Qi","executionInfo":{"status":"ok","timestamp":1734632688776,"user_tz":-330,"elapsed":132621,"user":{"displayName":"UTKARSH KHARAT","userId":"15647262042698647088"}},"outputId":"4141ec0e-1b12-426a-b35e-924b78da8bd2"},"id":"cychSa2PO5Qi","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluating the model on the test data...\n","\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 742ms/step - accuracy: 0.8716 - loss: 0.7620\n","Test Loss: 0.7658\n","Test Accuracy: 0.8714\n"]}]},{"cell_type":"code","source":["# Save the trained model to a file\n","model_path = '/content/drive/MyDrive/Project/nmt_model.h5'\n","\n","nmt_model.save(model_path)  # Save the model in H5 format\n","print(f\"Model saved successfully to {model_path}\")\n"],"metadata":{"id":"nsea3117QC56","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734632689454,"user_tz":-330,"elapsed":681,"user":{"displayName":"UTKARSH KHARAT","userId":"15647262042698647088"}},"outputId":"d12b7800-d1ef-441c-a11b-01ebe99d9626"},"id":"nsea3117QC56","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Model saved successfully to /content/drive/MyDrive/Project/nmt_model.h5\n"]}]},{"cell_type":"code","source":["# Load the saved model\n","from tensorflow.keras.models import load_model\n","\n","print(\"Loading the model...\")\n","loaded_model = load_model(model_path)\n","\n","print(\"Model loaded successfully!\")\n","loaded_model.summary()  # Display the model structure\n"],"metadata":{"id":"WcDW1UzfQKQR","colab":{"base_uri":"https://localhost:8080/","height":228},"executionInfo":{"status":"error","timestamp":1735060535762,"user_tz":-330,"elapsed":406,"user":{"displayName":"UTKARSH KHARAT","userId":"15647262042698647088"}},"outputId":"0c4c2bc4-4d9d-4506-c0f8-313b0a5674fb"},"id":"WcDW1UzfQKQR","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading the model...\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'model_path' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-55c88ea6ecf4>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading the model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model loaded successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model_path' is not defined"]}]},{"cell_type":"code","source":["#Define Inference Models\n","# Extract encoder layers\n","encoder_inputs = loaded_model.input[0]  # Encoder input\n","encoder_outputs, state_h, state_c = loaded_model.layers[4].output  # Encoder LSTM outputs and states\n","encoder_states = [state_h, state_c]\n","\n","# Define encoder model for inference\n","encoder_model = Model(encoder_inputs, encoder_states)\n","\n","# Extract decoder layers\n","latent_dim = 256  # Latent dimension size (same as training)\n","decoder_inputs = loaded_model.input[1]  # Decoder input\n","decoder_embedding = loaded_model.layers[3](decoder_inputs)  # Embedding layer\n","decoder_lstm = loaded_model.layers[5]  # Decoder LSTM layer\n","decoder_dense = loaded_model.layers[6]  # Dense output layer\n","\n","# Define placeholders for decoder states\n","decoder_state_input_h = Input(shape=(latent_dim,), name='input_h')\n","decoder_state_input_c = Input(shape=(latent_dim,), name='input_c')\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","\n","# Reuse decoder LSTM and Dense layers\n","decoder_outputs, state_h, state_c = decoder_lstm(\n","    decoder_embedding, initial_state=decoder_states_inputs\n",")\n","decoder_states = [state_h, state_c]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","# Define decoder model for inference\n","decoder_model = Model(\n","    [decoder_inputs] + decoder_states_inputs,\n","    [decoder_outputs] + decoder_states\n",")\n","\n","print(\"Inference models created successfully!\")\n"],"metadata":{"id":"qV5m9w9_Qr7N","colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"status":"error","timestamp":1735060539971,"user_tz":-330,"elapsed":490,"user":{"displayName":"UTKARSH KHARAT","userId":"15647262042698647088"}},"outputId":"eea640fc-0507-48fa-c2bb-195b3d4f3a0c"},"id":"qV5m9w9_Qr7N","execution_count":4,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'loaded_model' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-70c7e5c8314e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Define Inference Models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Extract encoder layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mencoder_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Encoder input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m  \u001b[0;31m# Encoder LSTM outputs and states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mencoder_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'loaded_model' is not defined"]}]},{"cell_type":"code","source":["#Prepare Reverse Tokenizer for Decoding\n","# Reverse-lookup dictionaries for decoding sequences back to words\n","reverse_eng_index = {v: k for k, v in eng_tokenizer.word_index.items()}  # English\n","reverse_fr_index = {v: k for k, v in fr_tokenizer.word_index.items()}  # French\n"],"metadata":{"id":"MNWerQ3TQ1Bv"},"id":"MNWerQ3TQ1Bv","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Define the Translation Function\n","def translate_sentence(input_sentence, max_len=20):\n","    \"\"\"\n","    Translate an input English sentence to French using the trained NMT model.\n","    \"\"\"\n","    # Preprocess the input sentence\n","    input_sentence = input_sentence.lower().strip()  # Normalize input\n","    input_sequence = eng_tokenizer.texts_to_sequences([input_sentence])  # Convert to sequence\n","    input_sequence = pad_sequences(input_sequence, maxlen=max_len, padding='post')  # Pad the sequence\n","\n","    # Encode the input sentence to get states\n","    states_value = encoder_model.predict(input_sequence)\n","\n","    # Initialize target sequence with <start> token\n","    target_seq = np.zeros((1, 1))\n","    target_seq[0, 0] = fr_tokenizer.word_index['je']  # Replace 'start' with your actual start token\n","\n","    # Generate the translated sentence\n","    stop_condition = False\n","    decoded_sentence = \"\"\n","    while not stop_condition:\n","        # Predict the next word and update states\n","        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n","\n","        # Get the word index with highest probability\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_word = reverse_fr_index.get(sampled_token_index, '')\n","\n","        # Stop if <end> token or max length reached\n","        if sampled_word == \"salutations\" or len(decoded_sentence.split()) >= max_len:\n","            stop_condition = True\n","        else:\n","            decoded_sentence += ' ' + sampled_word\n","\n","        # Update the target sequence and states\n","        target_seq[0, 0] = sampled_token_index\n","        states_value = [h, c]\n","\n","    return decoded_sentence.strip()\n"],"metadata":{"id":"cutRK3fOQ92n"},"id":"cutRK3fOQ92n","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.models import load_model\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Dropout\n","import numpy as np\n","import os\n","\n","# Tokenizers for English and French\n","eng_tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n","french_tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n","\n","# Check if tokenizer files exist\n","eng_tokenizer_path = \"/content/drive/MyDrive/Project/eng_tokenizer.pkl\"\n","fr_tokenizer_path = \"/content/drive/MyDrive/Project/french_tokenizer.pkl\"\n","\n","if os.path.exists(eng_tokenizer_path) and os.path.exists(fr_tokenizer_path):\n","    print(\"Tokenizer files found. Loading...\")\n","    # Load tokenizers using pickle\n","    with open(eng_tokenizer_path, \"rb\") as eng_file, open(fr_tokenizer_path, \"rb\") as fr_file:\n","        eng_tokenizer = pickle.load(eng_file)\n","        french_tokenizer = pickle.load(fr_file)\n","else:\n","    print(\"Tokenizer files not found in the specified location.\")\n","    print(\"Please make sure the paths are correct and the files exist.\")\n","    # Handle the case where files are not found (e.g., exit or raise an exception)\n","    # ... your error handling code here ...\n","\n","\n","# Load tokenizers (replace these paths with actual tokenizer files)\n","# Assume you have saved tokenizer configs during training\n","# Load tokenizers\n","# Update file paths to your Google Drive location\n","with open(\"/content/drive/MyDrive/Project/eng_tokenizer.pkl\", \"rb\") as eng_file, open(\"/content/drive/MyDrive/Project/fr_tokenizer.pkl\", \"rb\") as fr_file:\n","    eng_tokenizer = pickle.load(eng_file)  # Load using pickle\n","    french_tokenizer = pickle.load(fr_file)  # Load using pickle\n","\n","# ... other code ...\n","\n","# Create a target sequence with the start token\n","# Check if 'je' is in word_index, if not, add it\n","start_token = 'je' # Set start_token to 'je'\n","if start_token not in french_tokenizer.word_index:\n","    # Get the next available index (max existing index + 1)\n","    next_index = max(french_tokenizer.word_index.values(), default=0) + 1\n","    french_tokenizer.word_index[start_token] = next_index\n","\n","target_seq = np.zeros((1, 1))\n","target_seq[0, 0] = french_tokenizer.word_index[start_token]\n","\n","# Example sentences to translate\n","example_sentences = [\n","    \"hello\",\n","    \"What is your name?\",\n","    \"I love learning languages.\",\n","    \"Have a great day!\"\n","]\n","\n","# Function to prepare input sequence for the model\n","def prepare_sequence(input_sentence, tokenizer, max_length):\n","    input_sequence = tokenizer.texts_to_sequences([input_sentence])\n","    padded_sequence = pad_sequences(input_sequence, maxlen=max_length, padding=\"post\")\n","    return padded_sequence\n","\n","# Function to decode prediction back to text\n","def decode_sequence(predicted_sequence, tokenizer):\n","    word_index = {idx: word for word, idx in tokenizer.word_index.items()}\n","    return \" \".join([word_index.get(idx, \"\") for idx in predicted_sequence if idx != 0])\n","\n","# Translate each sentence\n","max_input_length = 20  # Replace with the actual max length used in training\n","max_output_length = 20  # Replace with the actual max length used in training\n","\n","for sentence in example_sentences:\n","    prepared_input = prepare_sequence(sentence.lower(), eng_tokenizer, max_input_length)\n","\n","    # Get the initial state for the decoder using the encoder model\n","    initial_state = encoder_model.predict(prepared_input)\n","\n","    # Create a target sequence with the start token\n","    # Check if 'start' is in word_index, if not use 'je' as start token\n","    start_token = '<start>' if '<start>' in french_tokenizer.word_index else 'je'\n","    target_seq = np.zeros((1, 1))\n","    target_seq[0, 0] = french_tokenizer.word_index[start_token]\n","\n","    # Generate the translation\n","    decoded_sentence = ''\n","    for _ in range(max_output_length):\n","        output_tokens, h, c = decoder_model.predict([target_seq] + initial_state)\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_word = reverse_fr_index.get(sampled_token_index, '')\n","\n","        # Check for end token or max length\n","        end_token = '<end>' if '<end>' in french_tokenizer.word_index else 'salutations'\n","        if sampled_word == end_token or len(decoded_sentence.split()) >= max_output_length:\n","            break\n","\n","        decoded_sentence += ' ' + sampled_word\n","        target_seq = np.zeros((1, 1))\n","        target_seq[0, 0] = sampled_token_index\n","        initial_state = [h, c]\n","\n","    translation = decoded_sentence.strip()\n","    print(f\"English: {sentence}\")\n","    print(f\"French: {translation}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":263},"id":"qYrTOMnIWk_Q","executionInfo":{"status":"error","timestamp":1735060512122,"user_tz":-330,"elapsed":6790,"user":{"displayName":"UTKARSH KHARAT","userId":"15647262042698647088"}},"outputId":"3b831aaf-4b2f-4124-c24f-3649fe426c02"},"id":"qYrTOMnIWk_Q","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenizer files not found in the specified location.\n","Please make sure the paths are correct and the files exist.\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/Project/eng_tokenizer.pkl'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-f323efa2a987>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Load tokenizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Update file paths to your Google Drive location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Project/eng_tokenizer.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0meng_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Project/fr_tokenizer.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfr_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0meng_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meng_file\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Load using pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mfrench_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfr_file\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Load using pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Project/eng_tokenizer.pkl'"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"1mrc2qsLNxfzGxbiWbh6ae8xV-dVSrZwV","timestamp":1732812956984}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.0"}},"nbformat":4,"nbformat_minor":5}